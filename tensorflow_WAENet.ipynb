{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#自编码网络\" data-toc-modified-id=\"自编码网络-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>自编码网络</a></span><ul class=\"toc-item\"><li><span><a href=\"#逐层学习策略\" data-toc-modified-id=\"逐层学习策略-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>逐层学习策略</a></span></li><li><span><a href=\"#稀疏编码\" data-toc-modified-id=\"稀疏编码-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>稀疏编码</a></span></li><li><span><a href=\"#卷积稀疏编码\" data-toc-modified-id=\"卷积稀疏编码-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>卷积稀疏编码</a></span></li><li><span><a href=\"#反卷积神经网络\" data-toc-modified-id=\"反卷积神经网络-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>反卷积神经网络</a></span></li><li><span><a href=\"#全卷积神经网络\" data-toc-modified-id=\"全卷积神经网络-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>全卷积神经网络</a></span></li></ul></li><li><span><a href=\"#自编码网络\" data-toc-modified-id=\"自编码网络-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>自编码网络</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型\" data-toc-modified-id=\"模型-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>模型</a></span></li><li><span><a href=\"#自编码网络的常见范式\" data-toc-modified-id=\"自编码网络的常见范式-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>自编码网络的常见范式</a></span><ul class=\"toc-item\"><li><span><a href=\"#稀疏自编码\" data-toc-modified-id=\"稀疏自编码-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>稀疏自编码</a></span></li><li><span><a href=\"#卷积自编码\" data-toc-modified-id=\"卷积自编码-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>卷积自编码</a></span></li><li><span><a href=\"#降噪自编码\" data-toc-modified-id=\"降噪自编码-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>降噪自编码</a></span></li><li><span><a href=\"#可收缩自编码\" data-toc-modified-id=\"可收缩自编码-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>可收缩自编码</a></span></li></ul></li></ul></li><li><span><a href=\"#深度堆栈网络\" data-toc-modified-id=\"深度堆栈网络-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>深度堆栈网络</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型\" data-toc-modified-id=\"模型-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>模型</a></span></li></ul></li><li><span><a href=\"#深度置信网络/深度boltzman机\" data-toc-modified-id=\"深度置信网络/深度boltzman机-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>深度置信网络/深度boltzman机</a></span><ul class=\"toc-item\"><li><span><a href=\"#boltzman机/受限boltzman机\" data-toc-modified-id=\"boltzman机/受限boltzman机-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>boltzman机/受限boltzman机</a></span></li><li><span><a href=\"#深度boltzman机/深度置信网络\" data-toc-modified-id=\"深度boltzman机/深度置信网络-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>深度boltzman机/深度置信网络</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型\" data-toc-modified-id=\"模型-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>模型</a></span></li><li><span><a href=\"#优化目标函数\" data-toc-modified-id=\"优化目标函数-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>优化目标函数</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 自编码网络\n",
    "深度网络随着层级的加深,导致非凸优化问题,使模型依赖于初值的选择,如何避免可行域的大量鞍点与局部极小值点?Hinton提出**使用无监督方法优化网络权值的初值,再使用少量有类标的数据对权值进行微调** \n",
    "### 逐层学习策略\n",
    "逐层学习策略是对神经网络层级间的参数进行‘剖分’式学习，即将相邻层级视为浅层神经网络，可充分发挥浅层神经网络的学习优势（凸优化），进一步，获取初始化参数后的层级通过‘复合’（堆栈）形成深度神经网络，可以大大地节省计算存储资源和时间，提高网络模型的泛化能力。通常基于逐层学习策略的参数初始化方法包括：分析形式（独立主成分分析，主成分分析）；合成形式（稀疏编码/稀疏表示，卷积稀疏编码），分析合成形式（基于三层前馈神经网络的各种自编码网络，boltzman机和受限boltzman机）<br/>\n",
    "深度神经网络参数初始化的方式还有非学习方式下的选取：如基于Gobor变换、小波变换、多尺度几何分析等构造滤波器组的集合，随机从该滤波器组集合中选取若干滤波器，赋值给层级间的权值矩阵；以及服从某种分布下的参数半随机化赋值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 稀疏编码\n",
    "Sparse Coding，通俗的说，就是将一个信号表示为一组基的线性组合，而且要求只需要较少的几个基就可以将信号表示出来。“稀疏性”定义为：只有很少的几个非零元素或只有很少的几个远大于零的元素。要求系数$a_i$是稀疏的意思就是说：对于一组输入向量，我们只想有尽可能少的几个系数远大于零。稀疏编码算法是一种无监督学习方法，它用来寻找一组“超完备”基向量来更高效地表示样本数据。虽然形如主成分分析技术（PCA）能使我们方便地找到一组“完备”基向量，但是这里我们想要做的是找到一组“超完备”基向量来表示输入向量（也就是说，基向量的个数比输入向量的维数要大）。超完备基的好处是它们能更有效地找出隐含在输入数据内部的结构与模式。然而，对于超完备基来说，系数$a_i$不再由输入向量唯一确定。因此，在稀疏编码算法中，我们另加了一个评判标准“稀疏性”来解决因超完备而导致的退化（degeneracy）问题。\n",
    "### 卷积稀疏编码\n",
    "由反卷积操作求解滤波器，且滤波器具有稀疏性。由此特征图也具有稀疏性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 反卷积神经网络\n",
    "反卷积可视化以各层得到的特征图作为输入，进行反卷积，得到反卷积结果，用以验证显示各层提取到的特征图。举个例子：假如你想要查看Alexnet 的conv5提取到了什么东西，我们就用conv5的特征图后面接一个反卷积网络，然后通过：反池化、反激活、反卷积，这样的一个过程，把本来一张$13\\times13$大小的特征图(conv5大小为$13\\times13$)，放大回去，最后得到一张与原始输入图片一样大小的图片($227\\times227$)。\n",
    "https://blog.csdn.net/lemianli/article/details/53171951 \n",
    "### 全卷积神经网络\n",
    "FCN对图像进行像素级的分类，从而解决了语义级别的图像分割（semantic segmentation）问题。与经典的CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类（全联接层＋softmax输出）不同，FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。最后逐个像素计算softmax分类的损失, 相当于每一个像素对应一个训练样本。\n",
    "https://blog.csdn.net/xiaojiajia007/article/details/54944023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 自编码网络\n",
    "自编码网络是指保持输入与输出尽可能一致的情形下，实现无监督方式下的**隐层特征提取与参数学习**其核心在于训练方式--浅层神经网络（凸优化理论），以及刻画方式--隐层特征的维数（通常升维对应稀疏性，降维对应压缩）。不论是中间隐层的特征维数上升或下降，对应的网络层级结构都是前向传播、拓扑无环结构。\n",
    "### 模型\n",
    "$$\\underset{\\theta}{min}\\quad J(\\theta)=\\frac{1}{N}\\sum_{n=1}^N{||\\boldsymbol{\\hat{x}}^{(n)}-\\boldsymbol{x}^{(n)}||_2^2}+\\lambda(||\\boldsymbol{W}_a||^2_F+||\\boldsymbol{W}_s||^2_F)$$\n",
    "\n",
    "$$\\begin{cases}\\boldsymbol{X}=\\sigma_a(\\boldsymbol{W}_a\\cdot \\boldsymbol{x}+\\boldsymbol{b}_a)\\in\\mathbb{R}^v\\\\\\hat{\\boldsymbol{x}}=\\sigma_s(\\boldsymbol{W}_s\\cdot \\boldsymbol{X}+\\boldsymbol{b}_s)\\in\\mathbb{R}^u\\end{cases}$$\n",
    "自编码网络的核心是基于有效的准则建立合理的损失项以期输入与其编码特征（即隐层输出）具有良好的拓扑结构对应性，进一步，其编码特征可以作为新的输入，利用同样的方式，得到对应的编码特征，依次循环，最终堆栈得到深度神经网络，这里的编码特征额可以视为输入的一种合理描述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 自编码网络的常见范式\n",
    "一种是编码特征可以较好地重构输入（例如稀疏自编码、卷积自编码）；二是对输入一定程度下的扰动具有不变性（例如降噪自编码、可收缩自编码）。\n",
    "#### 稀疏自编码\n",
    "稀疏自编码是指隐层特征具有稀疏响应特性（注意不限制隐层特征的维数一定大于输入信号的维数，可参考稀疏编码和稀疏表示之间的关系）通常稀疏性引入带来的优势有：一是编码方案存储能力大，具有联想记忆能力，并且计算简便；二是使自然信号结构更加清晰；三是编码方案既符合生物进化普遍的能量最小经济策略，又满足电生理实验的结论。<br/>\n",
    "通常引入稀疏性的方式有：一是不考虑隐层特征的维数与输入维数之间的关系，**利用KL距离引入稀疏性约束**；二是要求隐层特征的维数大于输入的维数，利用**伪范数$L_p(p\\in[0,1))$或范数$L_1$正则项引入稀疏性**。<br/>\n",
    "首先，隐层特征的输出可表示为：$$\\{\\boldsymbol{x}^{(n)}\\in\\mathbb{R}^u\\}^N_{n=1}\\rightarrow \\{\\boldsymbol{X}^{(n)}\\in\\mathbb{R}^v\\}_{n=1}^N$$\n",
    "隐层每个节点的平均值：$$\\bar{\\boldsymbol{X}}=\\frac{1}{N}\\sum_{n=1}^N{\\boldsymbol{X}^{(n)}}\\in\\mathbb{R}^v$$期望隐层每个节点的平均输出尽量为0，大部分的隐层节点处于静默状态\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "为了量化隐层的稀疏性，通常假设隐层每个节点以一定的小概率进行响应，且节点之间相互独立，假设隐层节点响应的概率为$\\rho$,利用KL构造的稀疏正则项为：$$KL(\\rho||\\bar{X}(j))=\\rho\\cdot log\\left(\\frac{\\rho}{X(j)}\\right)+(1-\\rho)\\cdot log\\left(\\frac{1-\\rho}{1-\\bar{X}(j)}\\right)$$其中，$\\bar{X}(j)$为$\\bar{X}$的第j个元素，即隐层第j个节点响应的平均值，其中j=1,...,v。在KL正则项的基础上，得到稀疏自编码网络的优化目标为：\n",
    "$$\\underset{\\theta}{min}\\quad J(\\theta)=\\frac{1}{N}\\sum_{n=1}^N||\\hat{\\boldsymbol{x}}^{(n)}-\\boldsymbol{x}^{(n)}||^2_2+\\lambda\\cdot R(\\theta)+\\beta\\cdot \\sum_{j=1}^v{KL(\\rho||\\bar{X}(j))}$$而在$L_p$或$L_1$稀疏正则项的基础上，仅需将稀疏正则项变为\n",
    "$$\\begin{cases}\\frac{1}{N}\\sum_{n=1}^N{||\\boldsymbol{X}^{(n)}||_1}\\\\\\frac{1}{N}\\sum_{n=1}^N{\\boldsymbol{X}^{(n)}||_0}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 卷积自编码\n",
    "卷积自编码网络的核心是在层级连接之间**引入卷积操作，改变普通自编码网络中的全连接模式**，$$\\begin{cases}\\boldsymbol{X}=\\sigma_a(\\boldsymbol{W}_a*\\boldsymbol{x}+\\boldsymbol{b}_a)\\in\\mathbb{R}^v\\\\\n",
    "\\hat{\\boldsymbol{x}}=\\sigma_s(\\boldsymbol{W}_s*\\boldsymbol{X}+\\boldsymbol{b}_s)\\in\\mathbb{R}^u\n",
    "\\end{cases}$$ 其中,“\\*”为卷积操作，原来层级间的全连接变为局部连接。 \n",
    "卷积自编码网络的输入不再限制为一维向量，对于二维图片也可以进行操作，除了局部连接特性以外，还可以引入权值共享机制。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 降噪自编码\n",
    "为学习到较为鲁棒性的特征,可以对输入数据(在网络中也称为可视层)**引入随机（加性）噪声**，此时，对于自编码网络而言，输入为带有噪声的数据，期望输出是没有噪声的数据。需要指出的是，这里带噪声的数据可以理解为对数据进行某种已知的退化操作所得到的。期望网络带有鲁棒性的原因之一是：普通自编码网络的缺点是当训练样本与测试样本不符合同一分布时，刻画的特征较差。<br/>\n",
    "相应的模型只需将$\\boldsymbol{x}$改为：$$\\tilde{\\boldsymbol{x}}^{(n)}=\\boldsymbol{x}^{(n)}+\\boldsymbol{\\epsilon}^{(n)}$$\n",
    "而优化目标仍为：$$\\underset{\\theta}{min}\\quad J(\\theta)=\\frac{1}{N}\\sum_{n=1}^N||\\hat{\\boldsymbol{x}}^{(n)}-\\boldsymbol{x}^{(n)}||^2_2+\\lambda\\cdot R(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 可收缩自编码\n",
    "可收缩自编码修正了普通自编码网络中的正则项，不直接对层级之间的连接矩阵进行惩罚，而是**利用隐层的输出关于输入的雅可比矩阵来进行惩罚**。其中雅可比矩阵可表示为：$$J_X(x)=\\left[\\frac{\\partial X(j)}{\\partial x(i)}\\right]_{u\\times v}$$\n",
    "对于雅可比矩阵的正则项约束可以**抑制训练样本在所有方向上的扰动**。<br/>\n",
    "可收缩自编码网络的优化目标可表示为：$$\\underset{\\theta}{min}J(\\theta)=\\frac{1}{N}\\sum_{n=1}^N{||\\hat{\\boldsymbol{x}}^{(n)}-\\boldsymbol{x}^{(n)}||^2_2}+\\lambda \\cdot ||J_X(x)||_2^F$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 深度堆栈网络\n",
    "深度堆栈网络是指**基于前馈神经网络的架构，其中相邻层级之间的（参数）学习采用自编码网络来实现，最后将自编码网络的分析（编码）部分拿出来以堆栈的形式形成的网络。**<br/>\n",
    "通常对于分类问题，网络的设计由两部分组成，**一部分为特征学习，另一部分为分类器设计**。具体操作包括逐层学习（参数的初始化策略）和精调（对深度前馈神经网络整体作端到端的微调）。为此其数据集分为两部分，一部分为少量有标记的样本，另一部分为大量无标记的样本。\n",
    "### 模型\n",
    "模型首先设置超参数：在特征学习阶段，有隐层层数L，每个隐层的节点数$n_l$每个隐层的激活函数$\\sigma_l(\\cdot)$；在分类器涉及阶段有分类器类型（如softmax,SVM,SMM)。<br/>\n",
    "自编码网络用于参数的初始化，最后仍进行正常的优化即得到深度堆栈网络。<br/>\n",
    "进一步提升网络堆栈网络性能的方法包括：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 深度置信网络/深度boltzman机\n",
    "### boltzman机/受限boltzman机\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度boltzman机/深度置信网络\n",
    "深度堆栈网络的核心是逐层训练与精调，也是目前半监督学习的主流，无论是**基于能量的自编码网络，还是基于信息量的(受限)boltzman机**，针对这一类网络模型的改进和应用都十分广泛。深度置信网络基于受限boltzman机，网络的整体结构仍为深度前馈网络，**只是层级间的初始化利用受限boltman机获取**，即将受限boltman机中的（隐层）乘兴偏置和权值连接矩阵直接赋值给相应层级的权值矩阵和偏置。\n",
    "#### 模型\n",
    "模型与传统自编码网络形成的深度堆栈网络唯一的区别就是在特征学习阶段，层级间参数初始化的网络模型为受限boltzman机：\n",
    "$$\\begin{cases}\\boldsymbol{h}\\tilde{} P(\\boldsymbol{h|v,W,a,b})=\\frac{1}{P(\\boldsymbol{v}\\cdot Z)}\\cdot e^{(\\boldsymbol{b}^T\\cdot \\boldsymbol{h}+\\boldsymbol{v}^T\\boldsymbol{\\cdot w\\cdot n})}\\\\ \\hat{v}\\tilde{} P(\\boldsymbol{v|h,W,a,b})=\\frac{1}{P(\\boldsymbol{v}\\cdot Z)}\\cdot e^{(\\boldsymbol{a}^T\\cdot \\boldsymbol{v}+\\boldsymbol{v}^T\\boldsymbol{\\cdot w\\cdot n})}\\end{cases}$$其中$\\boldsymbol{v}$为可视层，即输入进行归一化后的数据；$\\boldsymbol{h}$为隐层，服从相应的分布，可以由Gibbs采样获取。<br/>\n",
    "#### 优化目标函数\n",
    "$$\\begin{cases}\\{\\tilde{\\boldsymbol{x}}^{(t)}\\}_{t=1}^T\\overset{normalization}{\\rightarrow}\\{\\boldsymbol{v}^{(t)}\\}_{t=1}^T\\\\\n",
    "\\underset{\\theta}{min}J(\\theta)=-\\sum_{t=1}^T{logP(\\hat{\\boldsymbol{v}}^{(t)}}=-\\sum_{t=1}^T{log\\sum_h{P(\\hat{\\boldsymbol{v}}^{(t)},\\boldsymbol{h})}}\\end{cases}$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
