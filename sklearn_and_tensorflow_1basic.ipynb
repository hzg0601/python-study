{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 机器学习分类\n",
    "#### 监督 vs 无监督 vs 半监督 vs 强化学习\n",
    "#### 批学习 vs 在线学习：是否能进行增量学习\n",
    "#### 基于样本 vs 基于模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantecon as qe\n",
    "import numpy as np\n",
    "import os \n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = \"datasets/housing\"\n",
    "HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "fetch_housing_data()\n",
    "housing=load_housing_data()\n",
    "\n",
    "#descreption\n",
    "housing.head()\n",
    "housing.info()\n",
    "housing.describe()\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50,figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def split_train_test(data,test_ratio):\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices=np.random.permutation(len(data))\n",
    "    test_set_size=int(len(data)*test_ratio)\n",
    "    test_indices=shuffled_indices[:test_set_size]\n",
    "    train_indices=shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices],data.iloc[test_indices]\n",
    "#iloc return the whole data with train_indices\n",
    "train_set,test_set=split_train_test(housing,0.2)\n",
    "\n",
    "\n",
    "\n",
    "#both these solutions will break next time you fetch an updataed dataset\n",
    "#compute a hash of each instance's indentifier to decide whether\n",
    "#or not it shold go in the test set.\n",
    "#keep only the last byte of the hash,and put the instance in the test\n",
    "#set will remain consistent across multiple runs.\n",
    "#even if you refresh the dataset the new test set will contains 20%\n",
    "#of the new instances\n",
    "import hashlib\n",
    "def test_set_check(identifier,test_ratio,hash):\n",
    "    return hash(np.int64(identifier)).digest()[-1]<256*test_ratio\n",
    "def split_train_test_by_id(data,test_ratio,id_column,hash=hashlib.md5):\n",
    "    ids=data[id_column]\n",
    "    in_test_set=ids.apply(lambda id_:test_set_check(id_,test_ratio,hash))\n",
    "    return data.loc[~in_test_set],data.loc[in_test_set]\n",
    "#reset index with number\n",
    "housing_with_id=housing.reset_index()\n",
    "train_set,test_set=split_train_test_by_id(housing_with_id,0.2,'index')\n",
    "\n",
    "#reset index with unique feature\n",
    "housing_with_id['id']=housing['longitude']*1000+housing['latitude']\n",
    "train_set,test_set=split_train_test_by_id(housing_with_id,0.2,'id')\n",
    "\n",
    "#split the date with sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set,test_set=train_test_split(housing,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#split dataset by median income\n",
    "\n",
    "housing['income_cat']=np.ceil(housing['median_income']/1.5)\n",
    "housing['income_cat'].where(housing['income_cat']<5,5,inplace=True)\n",
    "#keep elements meet condition(arg1),others are replaced by arg2\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for train_index,test_index in split.split(housing,housing['income_cat']):\n",
    "    strat_train_set=housing.loc[train_index]\n",
    "    strat_test_set=housing.loc[test_index]\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#StratifiedShuffleSplit.split 返回分割结果的索引（数字）\n",
    "#loc 按数字索引查询全部数据\n",
    "housing['income_cat'].value_counts()/len(housing)\n",
    "for set in (strat_train_set,strat_test_set):\n",
    "    set.drop(['income_cat'],axis=1,inplace=True)\n",
    "    \n",
    "housing=strat_train_set.copy()\n",
    "#深复制\n",
    "housing.plot(kind='scatter',x='longitude',y='latitude',c='blue',alpha=0.2)\n",
    "\n",
    "housing.plot(kind='scatter',x='longitude',y='latitude',alpha=0.4,\n",
    "            s=housing['population']/100,label='population',\n",
    "            c='median_house_value',cmap=plt.get_cmap('jet'),colorbar=True)\n",
    "plt.legend()\n",
    "plt.show()    \n",
    "    \n",
    "corr_matrix=housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "attributes=['median_house_value','median_income','total_rooms','housing_median_age']\n",
    "scatter_matrix(housing[attributes],figsize=(12,8))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing=strat_train_set.drop('median_house_value',axis=1)\n",
    "housing_labels=strat_train_set['median_house_value'].copy()\n",
    "housing.dropna(subset=['total_bedrooms'])#drop value\n",
    "housing.drop('total_bedrooms',axis=1)#drop feature\n",
    "median=housing['total_bedrooms'].median()\n",
    "housing['total_bedrooms'].fillna(median)#fill nan with median\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer=Imputer(strategy='median')\n",
    "housing_num=housing.drop('ocean_proximity',axis=1)\n",
    "imputer.fit(housing_num)#拟合而不应用\n",
    "imputer.statistics_#all features' median\n",
    "housing_num.median().values#all features' median\n",
    "X=imputer.transform(housing_num)\n",
    "housing_tr=pd.DataFrame(X,columns=housing_num.columns)\n",
    "\n",
    "#fit/Trasform/fit-transform()/predict/Inspection(strategy,statistics_) \n",
    "\n",
    "\n",
    "#handing text and categorical attributes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()#以单一数字编码\n",
    "housing_cat=housing['ocean_proximity']\n",
    "housing_cat_encoded=encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded\n",
    "encoder.classes_\n",
    "\n",
    "#以onehotEncoder将单一数字编码变为one-hot键编码,\n",
    "#形式为scipy sparse matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder=OneHotEncoder()\n",
    "housing_cat_1hot=encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "housing_cat_1hot\n",
    "\n",
    "#直接将文本变为one-not编码,默认为dense Numpy array,\n",
    "#取参数sparse_output=True 变为sparse matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder=LabelBinarizer()\n",
    "housing_cat_1hot=encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义类，以测试增加特征甚至改变超参数的效果\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "rooms_ix,bedrooms_ix,population_ix,household_ix=3,4,5,6\n",
    "class CombineAttributesAdder(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,add_bedrooms_per_room=True):#no *args or **kargs\n",
    "        self.add_bedrooms_per_room=add_bedrooms_per_room\n",
    "    def fit(self,X,y=None):\n",
    "        return self#nothing else to do\n",
    "    def transform(self,X,y=None):\n",
    "        rooms_per_household=X[:,rooms_ix]/X[:,household_ix]\n",
    "        population_per_household=X[:,population_ix]/X[:,household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room=X[:,bedrooms_ix]/X[:,rooms_ix]\n",
    "            return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X,rooms_per_household,population_per_household]\n",
    "attr_adder=CombineAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs=attr_adder.transform(housing.values)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical features as a numeric array.\n",
    "    The input to this transformer should be a matrix of integers or strings,\n",
    "    denoting the values taken on by categorical (discrete) features.\n",
    "    The features can be encoded using a one-hot aka one-of-K scheme\n",
    "    (``encoding='onehot'``, the default) or converted to ordinal integers\n",
    "    (``encoding='ordinal'``).\n",
    "    This encoding is needed for feeding categorical data to many scikit-learn\n",
    "    estimators, notably linear models and SVMs with the standard kernels.\n",
    "    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n",
    "        The type of encoding to use (default is 'onehot'):\n",
    "        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n",
    "          (or also called 'dummy' encoding). This creates a binary column for\n",
    "          each category and returns a sparse matrix.\n",
    "        - 'onehot-dense': the same as 'onehot' but returns a dense array\n",
    "          instead of a sparse matrix.\n",
    "        - 'ordinal': encode the features as ordinal integers. This results in\n",
    "          a single column of integers (0 to n_categories - 1) per feature.\n",
    "    categories : 'auto' or a list of lists/arrays of values.\n",
    "        Categories (unique values) per feature:\n",
    "        - 'auto' : Determine categories automatically from the training data.\n",
    "        - list : ``categories[i]`` holds the categories expected in the ith\n",
    "          column. The passed categories are sorted before encoding the data\n",
    "          (used categories can be found in the ``categories_`` attribute).\n",
    "    dtype : number type, default np.float64\n",
    "        Desired dtype of output.\n",
    "    handle_unknown : 'error' (default) or 'ignore'\n",
    "        Whether to raise an error or ignore if a unknown categorical feature is\n",
    "        present during transform (default is to raise). When this is parameter\n",
    "        is set to 'ignore' and an unknown category is encountered during\n",
    "        transform, the resulting one-hot encoded columns for this feature\n",
    "        will be all zeros.\n",
    "        Ignoring unknown categories is not supported for\n",
    "        ``encoding='ordinal'``.\n",
    "    Attributes\n",
    "    ----------\n",
    "    categories_ : list of arrays\n",
    "        The categories of each feature determined during fitting. When\n",
    "        categories were specified manually, this holds the sorted categories\n",
    "        (in order corresponding with output of `transform`).\n",
    "    Examples\n",
    "    --------\n",
    "    Given a dataset with three features and two samples, we let the encoder\n",
    "    find the maximum value per feature and transform the data to a binary\n",
    "    one-hot encoding.\n",
    "    >>> from sklearn.preprocessing import CategoricalEncoder\n",
    "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n",
    "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "    ... # doctest: +ELLIPSIS\n",
    "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n",
    "              encoding='onehot', handle_unknown='ignore')\n",
    "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n",
    "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
    "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "    See also\n",
    "    --------\n",
    "    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n",
    "      integer ordinal features. The ``OneHotEncoder assumes`` that input\n",
    "      features take on values in the range ``[0, max(feature)]`` instead of\n",
    "      using the unique values.\n",
    "    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
    "      dictionary items (also handles string-valued features).\n",
    "    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
    "      encoding of dictionary items or strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaling:min-max(normalization) and standardization\n",
    "#MinMaxScaler,feature_range控制是否在0-1之间\n",
    "#StandardScaler进行标准化，对异常值不敏感\n",
    "\n",
    "#管道流对数据同时进行多种操作 name/estimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_pipeline=Pipeline([('imputer',Imputer(strategy='median')),\n",
    "                      ('attribs_adder',CombineAttributesAdder()),\n",
    "                      ('std_scaler',StandardScaler())])\n",
    "#除最后一个外，其他必须为transformer,即必须有fit_transform方法\n",
    "housing_num_tr=num_pipeline.fit_transform(housing_num)\n",
    "#pipeline.fit从第一个开始执行fit_transform方法，至最后一个estimator只调用fit方法\n",
    "#而pipeline.fit_transform每一步都执行fit_transform方法\n",
    "\n",
    "#FeatureUnion类 将所有变形整合到一个管道流\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn_features.transformers import DataFrameSelector\n",
    "#from sklearn import CategoricalEncoder\n",
    "num_attribs=list(housing_num)\n",
    "cat_attribs=['ocean_proximity']\n",
    "num_pipeline=Pipeline([\n",
    "    ('selector',DataFrameSelector(num_attribs)),\n",
    "    ('imputer',Imputer(strategy='median')),\n",
    "    ('attribs_adder',CombineAttributesAdder()),\n",
    "    ('std_scaler',StandardScaler())\n",
    "])\n",
    "cat_pipeline=Pipeline([\n",
    "    ('selector',DataFrameSelector(cat_attribs)),\n",
    "    ('label_binarizer',CategoricalEncoder(encoding=\"onehot-dense\"))\n",
    "])\n",
    "\n",
    "full_pipeline=FeatureUnion(transformer_list=[\n",
    "    ('num_pipeline',num_pipeline),\n",
    "    ('cat_pipeline',cat_pipeline)\n",
    "])\n",
    "housing_prepared=full_pipeline.fit_transform(housing)\n",
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "class DataFrameSelector(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,attribute_names):\n",
    "        self.attribute_names=attribute_names\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return X[self.attribute_names].values\n",
    "    \n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "num_attribs=list(housing_num)\n",
    "cat_attribs=['ocean_proximity']\n",
    "num_pipeline=Pipeline([\n",
    "    ('selector',DataFrameSelector(num_attribs)),\n",
    "    ('imputer',Imputer(strategy='median')),\n",
    "    ('attribs_adder',CombineAttributesAdder()),\n",
    "    ('std_scaler',StandardScaler())\n",
    "])\n",
    "cat_pipeline=Pipeline([\n",
    "    ('selector',DataFrameSelector(cat_attribs)),\n",
    "    ('label_binarizer',LabelBinarizer())\n",
    "])\n",
    "\n",
    "full_pipeline=FeatureUnion(transformer_list=[\n",
    "    ('num_pipeline',num_pipeline),\n",
    "    ('cat_pipeline',cat_pipeline)\n",
    "])\n",
    "housing_prepared=full_pipeline.fit_transform(housing)\n",
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69052.46136345083\n",
      "70800.290109054\n",
      "52747.97760573926\n",
      "68628.19819848923 0.0 22249.136793360492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(housing_prepared,housing_labels)\n",
    "\n",
    "some_data=housing.iloc[:5]\n",
    "some_labels=housing_labels.iloc[:5]\n",
    "some_data_prepared=full_pipeline.transform(some_data)\n",
    "lin_reg.predict(some_data_prepared)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "housing_predictions=lin_reg.predict(housing_prepared)\n",
    "lin_mse=mse(housing_labels,housing_predictions)\n",
    "\n",
    "lin_rmse=np.sqrt(lin_mse)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg=DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared,housing_labels)\n",
    "housing_predictions=tree_reg.predict(housing_prepared)\n",
    "tree_mse=mse(housing_labels,housing_predictions)\n",
    "\n",
    "tree_rmse=np.sqrt(tree_mse)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score as cvs\n",
    "#将测试集分成10折，每次用9折训练1折学习，共10次，取10次结果的平均\n",
    "scores=cvs(tree_reg,housing_prepared,housing_labels,scoring='neg_mean_squared_error',cv=10)\n",
    "rmse_scores=np.sqrt(-scores)\n",
    "\n",
    "lin_scores=cvs(lin_reg,housing_prepared,housing_labels,scoring='neg_mean_squared_error',cv=10)\n",
    "lin_scores_rmse=np.sqrt(-lin_scores)\n",
    "print(lin_scores_rmse.mean())\n",
    "print(rmse_scores.mean())\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg=RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared,housing_labels)\n",
    "forest_reg_predictions=forest_reg.predict(housing_prepared)\n",
    "forest_mse=mse(forest_reg_predictions,housing_labels)\n",
    "forest_rmse=np.sqrt(forest_mse)\n",
    "\n",
    "\n",
    "forest_scores=cvs(forest_reg,housing_prepared,housing_labels,scoring='neg_mean_squared_error',cv=10)\n",
    "forest_socres_rmse=np.sqrt(-forest_scores)\n",
    "print(forest_socres_rmse.mean())\n",
    "\n",
    "print(lin_rmse,tree_rmse,forest_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7f12aca7b067>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m },{'bootstrap':[False],'n_estimator':[3,10],'max_features':[2,3,4]},]\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mforest_reg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforest_reg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhousing_prepared\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhousing_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "#调参\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid=[{\n",
    "    'n_estimators':[3,10,30],'max_features':[2,4,6,8]\n",
    "},{'bootstrap':[False],'n_estimator':[3,10],'max_features':[2,3,4]},]\n",
    "\n",
    "from skearn.ensemble import RandomForestRegressor\n",
    "forest_reg=RandomForestRegressor()\n",
    "grid_search=GridSearchCV(forest_reg,param_grid,cv=5,scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_prepared,housing_labels)\n",
    "\n",
    "grid_search.best_params_\n",
    "grid_search.best_estimator_\n",
    "\n",
    "cvres=grid_search.cv_results_\n",
    "for mean_score,params in zip(cvres['mean_test_score'],cvres['params']):\n",
    "    print(np.sqrt(-mean_score),params)\n",
    "    \n",
    "feature_importances=grid_search.best_estimator_.feature_importances_\n",
    "extra_attribs=['rooms_per_hhold','pop_per_hhold','bedrooms_per_room']\n",
    "cat_one_hot_attribs=list(encoder.classes_)\n",
    "attributes=num_attribs+extra_attribs+cat_one_hot_attribs\n",
    "sorted(zip(feature_importances,attributes),reverse=True)\n",
    "#如果参数空间过大，则需使用RandomizedSearchCV\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(grid_search,'my_model.txt')\n",
    "my_model_loaded=joblib.load('my_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试数据\n",
    "final_model=gird.search.best_estimator_\n",
    "X_test=strat_test_set.drop('meadian_housing_value',axis=1)\n",
    "y_test=strat_test_set['meadian_housing_value'].copy()\n",
    "\n",
    "X_test_prepared=full_pipeline.transform(X_test)\n",
    "final_predictions=final_model.predict(X_test_prepared)\n",
    "final_mse=mse(y_test,final_predictions)\n",
    "final_rmse=np.sqrt(final_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
