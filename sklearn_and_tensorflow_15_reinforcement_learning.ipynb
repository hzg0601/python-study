{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习\n",
    "在强化学习中，智能体agent在环境environment中做出观测observations做出动作actions,而后得的奖励rewards，行动的目的是获得最大期望长期受益。\n",
    "## 策略搜寻（policy search）\n",
    "策略:智能体决策行动的算法。策略可以是任何算法，甚至不是确定的算法。策略搜寻通常用遗传算法genetic algorithms。一次生成多个策略，然后按表现将策略排序，一一个特定的分位数舍去表现差的策略，让留下的策略组合产生新的子策略。（父辈的拷贝加入一些随机变换），留下的策略与子策略构成第二代策略集。然后迭代进行，直至找到最优策略。<br/>\n",
    "另一种常用的策略搜寻算法称为策略梯度算法。首先奖励对策略参数的梯度，然后根据梯度上升的方法调整参数.\n",
    "## OpenAI Gym简介\n",
    "在真实场景中训练强化学习是非常昂贵且费时的，因此训练通常是在模拟环境中进行的。OpenAI Gym是一个复杂而包罗万象的模拟环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gym' has no attribute 'make'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b6d62c38d742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CartPole-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gym' has no attribute 'make'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env=gym.make('CartPole-v0')\n",
    "obs=env.reset()\n",
    "env.render()\n",
    "img=env.render(mode='rgb_array')\n",
    "img.shape\n",
    "env.action_space\n",
    "\n",
    "action=1\n",
    "obs,reward,done,info=env.step(action)\n",
    "\n",
    "\n",
    "\n",
    "def basic_policy(obs):\n",
    "    angle=obs[2]\n",
    "    return 0 if angle<0 else 1\n",
    "total=[]\n",
    "for episode in range(500):\n",
    "    obs=env.reset()\n",
    "    for step in range(1000):\n",
    "        action=basic_policy(obs)\n",
    "        obs,reward,done,info=env.step(action)\n",
    "        episode_rewards+=reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.eppend(episode_rewards)\n",
    "    \n",
    "import numpy as np\n",
    "np.mean(totals),np.std(totals),np.min(totals),np.max(totals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络方法\n",
    "神经网络以观测为输入，动作为输出，更准确地说，是估计每个动作的概率，然后根据概率随机选择动作。在CartPole中，由于只有两个动作，所以只需要一个输出神经元，输出为action 0（左）该率P.根据概率随机选择的动作，而不是直接根据概率选择的动作的原因在于，这个方法可以使智能体在发掘新动作和利用现有动作之间取得平衡。需要注意的是，在这个特殊的环境中，过去的动作和观测，可以被忽略，因为每个动作都包含这个环境的完全状态。如果环境中存在隐藏动作，那么过去的动作和观测也需要被考虑进去。如：CartPole的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "n_inputs=4\n",
    "n_hidden=4\n",
    "n_outputs=1\n",
    "initializer=tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=[None,n_inputs])\n",
    "hidden=fully_connected(X,n_hidden,activation_fn=tf.nn.elu,weights_initializer=initializer)\n",
    "logits=fully_connected(hidden,n_outputs,activation_fn=None,weights_initializer=initializer)\n",
    "outputs=tf.nn.sigmoid(logits)\n",
    "\n",
    "p_left_and_right=tf.concat(value=[outputs,1-outputs],axis=1)\n",
    "action=tf.multinomial(tf.log(p_left_and_right),num_samples=1)#按概率随机选择动作，多项式随机\n",
    "init=tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 估计动作：信度指派问题\n",
    "\n",
    "如果知道每一步的最优动作，我们当然可以像训练监督学习那样训练神经网络。但问题在于，在强化学习中，智能体只能通过奖励活动反馈，而奖励是稀疏而且有延迟的。例如，如果要在100步内平衡杆子，它该如何知道100步中哪些是好的，哪些是坏的，而如果最终杆子在第100步时坠落，那么问题并不是完全出自第100步，这种问题即被称为信度指派问题。当智能体获得奖励，它并不能确定哪一步是可信的，哪一步是可疑的。<br/>\n",
    "解决该问题最常用的一种方法是：根据之后获得的奖励总和来估计该动作，通常会以一个折旧率来刻画时间价值。如一个连续三次向右的动作，第一次奖励为10，第二次奖励为0，最终奖励为-50，折旧率为0.8，则第一个动作的总得分为：$10+r\\times 0+r^2\\times(-50)=-22$,通常折旧率为0.95或0.99，折旧率为0.95时，10步后的奖励相当于即时奖励的一半，折旧率为0.99，相当于69步后的奖励相当于即时奖励的一半。在CartPole环境中，动作具有短期效应，选择0.95的折旧率是合理的。当然好的动作如果跟随的是一系列坏的动作，那么好的动作的得分也会很低，可是如果时间足够长的话，平均来说，好的动作的平均得分高于坏的动作。因此为得到相对可靠的动作得分，我们必须多次运行并标准化所有得分。最后，得分为负的为坏动作，为正的为好动作。\n",
    "## 策略梯度\n",
    "策略梯度算法指的是跟随更高得分的梯度从而优化策略的参数方法。最流行的策略梯度算法是REINFORCE算法，以下是它的一个变体：\n",
    "1. 首先，然神经网络策略测试游戏数次，并在每一步计算使选中的动作更可能的梯度，但并不应用这些梯度。\n",
    "1. 运行游戏数次，计算每个动作的得分。\n",
    "3. 如果某个动作的得分为正，则希望应用之前计算的梯度使得该动作更容易被选中，反正，则应用相反的梯度。方法就是将梯度向量与对应动作得分相乘。\n",
    "4. 最后，计算所有梯度向量的均值，并用其执行梯度下降方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=1.-tf.to_float(action)#目标概率\n",
    "learning_rate=0.01\n",
    "\n",
    "cross_entropy=tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "optimizer=AdamOptimizer(learning_rate=learning_rate)\n",
    "grad_and_vars=optimizer.compute_gradients(cross_entropy)#得到梯度和变量\n",
    "\n",
    "gradients=[grad for grad,variable in gard_and_vars]#取出梯度\n",
    "#在执行阶段，算法会运行所选择的策略，而在运行的每一步，算法也会评估这些梯度向量并且存储\n",
    "#向量的值。几次运行之后，算法会调整这些梯度（与动作得分相乘并标准化），并计算调整梯度的\n",
    "#均值。然后，将调整的梯度代入之前的优化器，最后执行优化过程，这意味着我们需要为每个梯度\n",
    "#向量设一个占位符。此外，我们还必须创建更新梯度的操作，为此我们要调用优化器的apply_gradients\n",
    "#函数，创建一个包含更新梯度的列表。\n",
    "gradient_placeholders=[]\n",
    "grads_and_vars_feed=[]\n",
    "for grad,variable in grads_and_vars:\n",
    "    gradient_placeholder=tf.placeholder(tf.float32,shape=grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grad_and_vars_feed.append((gradient_placeholder),variable)\n",
    "\n",
    "training_op=optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf.contrib.layers import fully_connected\n",
    "n_inputs=4\n",
    "n_hidden=4\n",
    "n_outputs=1\n",
    "initializer=tf.contrib.layers.variance_scaling_initializer()\n",
    "learning_rate=0.01\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=[None,n_inputs])\n",
    "hidden=fully_connected(X,n_hidden,activation_fn=tf.nn.elu,weights_initializer=initializer)\n",
    "logits=fully_connected(hidden,n_outputs,activaion_fn=None,weights_initializer=initializer)\n",
    "outputs=tf.nn.sigmoid(logits)\n",
    "\n",
    "p_left_and_right=tf.concat(axis=1,values=[outputs,1-outputs])\n",
    "action=tf.multinomial(tf.log(p_left_and_right),num_samples=1)\n",
    "y=1.-tf.to_float(action)\n",
    "cross_entropy=tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "grads_and_vars=optimizer.compute_gradients(cross_entropy)\n",
    "gradients=[grad for gard,variable in grads_and_vars]\n",
    "\n",
    "gradient_placeholders=[]\n",
    "grads_and_vars_feed=[]\n",
    "for grad,variable in grad_and_vars:\n",
    "    gradient_placeholder=tf.placeholder(tf.float32,shape=grad.get_shape())\n",
    "    gradient_placholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder,variable))\n",
    "training_op=optimizer.apply_gradents(grads_and_vars_feed)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n",
    "\n",
    "\n",
    "#执行阶段\n",
    "def discount_rewards(rewards,discount_rate):\n",
    "    discounted_rewards=np.empty(len(rewards))\n",
    "    cumulative_rewards=0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards=rewards[step]+cumulative_rewards*discount_rate\n",
    "        discounted_rewards[step]=cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards,discount_rate):\n",
    "    all_discounted_rewards=[discount_rewards(rewards) for rewards in all_rewards]\n",
    "    flat_rewards=np.concatenate(all_discounted_rewards)\n",
    "    reward_mean=flat_rewards.mean()\n",
    "    reward_std=flat_rewards.std()\n",
    "    return [(discounted_rewards-reward_mean)/reward_std\n",
    "           for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "\n",
    "n_iterations=250\n",
    "n_max_steps=1000\n",
    "n_games_per_update=10\n",
    "save_iterations=10\n",
    "discount_rate=0.95\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        all_rewards=[]\n",
    "        all_grdients=[]\n",
    "        for game in range(n_game_per_update):\n",
    "            current_rewards=[]\n",
    "            current_gradients=[]\n",
    "            obs=env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val,gradients_val=sess.run(\n",
    "                [action,gradients],feed_dict={X:obs.reshape(1,n_inputs)})#one obs\n",
    "                obs,reward,done,info=env.step(action_val[0][0])\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current__rewards)\n",
    "            all_gradients.append(current_graidents)\n",
    "            #运行10episodes,并更新策略\n",
    "            all_rewards=discount_and_nomalize_rewards(all_rewards)\n",
    "            feed_dict={}\n",
    "            for val_index,grad_placeholder in enumerate(gradient_placholders):\n",
    "                mean_gradients=np.mean(\n",
    "                [reward*all_gradients[game_index][step][var_index] for game_index,rewards \n",
    "                in enumerate(all_rewards) for step,reward in enumerate(rewards)],axis=0)\n",
    "                feed_dict[grad_placeholder]=mean_gradients\n",
    "            sess.run(training_op,feed_dict=feed_dict)\n",
    "            if iteration%save_iterations==0:\n",
    "                saver.save(sess,'./my_policy_net_pg.ckpt')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
