{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自编码网络\n",
    "自编码是一种无监督学习方式，一般输出比输入的维度要小的多，因此常用于降维问题。自编码是一种有效的特征检测工具，可用于深度网络的预训练。此外自编码网络还可以用于随机生成与输入数据及其相似的新数据，我们称为生成模型。因此其典型应用包括：降维，特征提取，无监督预训练，生成模型。<br/>\n",
    "自编码网络通常由两部分构成：编码器将输入转化成内部表示（称识别网络，通常低于输入的维数），解码器将内部表示转变为输出（称生成网络）。其网络结构类似于多层感知器MLP，但输入神经元与输出神经元的数目必须相等。输出通常也称重建，损失函数称为重建损失。由于内部表示的维度通常低于输出的维度，而损失函数要求输入输出想去不远，因而迫使网络学习输入最重要的特征。\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "n_input=3\n",
    "n_hidden=2\n",
    "n_output=n_input\n",
    "learning_rate=0.01\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=[None,n_input])\n",
    "hidden=fully_connected(X,n_hidden,activation_fn=None)\n",
    "output=fully_connected(hidden,n_output,activation_fn=None)\n",
    "reconstruction_loss=tf.reduce_mean(tf.square(output-X))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "training_op=optimizer.minimize(reconstruction_loss)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_train,y_train,X_test,y_test=tts(mnist[\"data\"],mnist['target'],test_size=0.2)\n",
    "n_iteration=1000\n",
    "coding=hidden#隐层的损失作为coding\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(n_iteration):\n",
    "        training_op.run(feed_dict={X:X_train})\n",
    "    coding_val=coding_eval(feed_dict={X:X_test})\n",
    "```\n",
    "activation_fn=None,损失函数为MSE即PCA。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-677744736d0b>:18: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-677744736d0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mtraining_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mcoding_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   2375\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2376\u001b[0m     \"\"\"\n\u001b[1;32m-> 2377\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2379\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gradient\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5213\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5214\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 5215\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "n_input=28*28\n",
    "n_hidden=2\n",
    "n_output=n_input\n",
    "learning_rate=0.01\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=[None,n_input])\n",
    "hidden=fully_connected(X,n_hidden,activation_fn=None)\n",
    "output=fully_connected(hidden,n_output,activation_fn=None)\n",
    "reconstruction_loss=tf.reduce_mean(tf.square(output-X))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "training_op=optimizer.minimize(reconstruction_loss)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_train=mnist.train.images\n",
    "X_test=mnist.test.images\n",
    "n_iteration=1000\n",
    "coding=hidden#隐层的损失作为coding\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(n_iteration):\n",
    "        training_op.run(feed_dict={X:X_train})\n",
    "    coding_val=coding.eval(feed_dict={X:X_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 堆栈自编码网络\n",
    "堆栈自编码网络通常是对称的，且深度有限以避免过拟合。\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/hzg0601/python/master/stacked_autoencoder.png' width='500'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "tf.reset_default_graph()\n",
    "n_input=28*28\n",
    "n_hidden1=300\n",
    "n_hidden2=150\n",
    "n_hidden3=n_hidden1\n",
    "n_output=n_input\n",
    "\n",
    "learning_rate=0.01\n",
    "l2_reg=0.001\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=[None,n_input])\n",
    "with tf.contrib.framework.arg_scope(\n",
    "[fully_connected],activation_fn=tf.nn.elu,\n",
    "    weights_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "    weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "):\n",
    "    hidden1=fully_connected(X,n_hidden1)\n",
    "    hidden2=fully_connected(hidden1,n_hidden2)\n",
    "    hidden3=fully_connected(hidden2,n_hidden3)\n",
    "    output=fully_connected(hidden3,n_output,activation_fn=None)\n",
    "    \n",
    "    reconstruction_loss=tf.reduce_mean(tf.square(output-X))\n",
    "    reg_losses=tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss=tf.add_n([reconstruction_loss]+reg_losses)\n",
    "    \n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op=optimizer.minimize(loss)\n",
    "    \n",
    "    init=tf.global_variables_initializer()\n",
    "n_epochs=5\n",
    "batch_size=150\n",
    " \n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches=mnist.train.num_examples//batch_size#取模\n",
    "        for i in range(n_batches):\n",
    "            X_batch,y_batch=mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op,feed_dict={X:X_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 权重关联\n",
    "对于对称自编码器，可以将解码层的权重与编码层的权重相关联，这种方法可以将权重的数量减半，从而加速训练，减少过拟合的风险。记$W_L$为L层的连接权重，假设模型共L层，则$\\frac{N}{2}$即编码层，N层为输出层，解码层的权重可定义为：$W_{N-L+1}=W_L^T$ .用fully_connected函数定义关联权重比较困难，可以手动定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_input=28*28\n",
    "n_hidden1=300\n",
    "n_hidden2=150\n",
    "n_hidden3=n_hidden1\n",
    "n_output=n_input\n",
    "\n",
    "activation=tf.nn.elu\n",
    "regularizer=tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "initializer=tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_inputs))\n",
    "weights1_init=initializer([n_inputs,n_hidden1])\n",
    "weights2_init=initializer([n_hidden1,n_hidden2])\n",
    "\n",
    "weights1=tf.Variable(weights1_init,dtype=tf.float32,name='weights1')\n",
    "weights2=tf.Variable(weights2_init,dtype=tf.float32,name='weights2')\n",
    "weights3=tf.transpose(weights2,name='weights3')\n",
    "weights4=tf.transpose(weights1,name='weights4')#权重关联\n",
    "\n",
    "biases1=tf.Variable(tf.zeros(n_hidden1),name='biases1')\n",
    "biases2=tf.Variable(tf.zeros(n_hidden2),name='biases2')\n",
    "biases3=tf.Variable(tf.zeros(n_hidden3),name='biases3')\n",
    "biases4=tf.Variable(tf.zeros(n_hidden4),name='biases4')\n",
    "\n",
    "hidden1=activation(tf.matmul(X,weights1)+biases1)\n",
    "hidden2=activation(tf.matmul(hidden1,weights2)+biases2)\n",
    "hidden3=activation(tf.matmul(hidden2,weights3)+biases3)\n",
    "outputs=tf.matmul(hidden3,weights4)+biases4\n",
    "\n",
    "reconstruction_loss=tf.reduce_mean(tf.square(output-X))\n",
    "reg_loss=regularizer(weights1)+regularizer(weights2)\n",
    "loss=reconstruction_loss+reg_loss\n",
    "\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "training_op=optimizer.minimizer(loss)\n",
    "\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 每次训练一个自编码器\n",
    "一次训练整个堆栈网络是非常耗时，因此常常是一次训练一个浅的自编码器，然后将其堆栈成一个网络。\n",
    "<img width='500' height='400' src='https://raw.githubusercontent.com/hzg0601/python/master/stacked_autoencoder_2.png'>\n",
    "\n",
    "第一阶段学习重建输入，第二阶段学习重建上一个的隐层。执行的方法是：一，每个阶段用不同的计算图；或者，二，用同一个计算图，如下图所示：\n",
    "<img width='500' height='400' src='https://raw.githubusercontent.com/hzg0601/python/master/stacked_autoencoder_3.png'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "with tf.name_scope('phase1'):\n",
    "    phase1_outputs=tf.matmul(hidden1,weights4)+biases4\n",
    "    phase1_reconstruction_loss=tf.reduce_mean(tf.square(phase1_outputs)-X)\n",
    "    phase1_reg_loss=regularizer(weights1)+regularizer(weights4)#隐层1和输出层\n",
    "    phase1_loss=phase1_reconstruction_loss+phase1_reg_loss\n",
    "    phase1_training_op=optimizer.minimize(phase1_loss)\n",
    "\n",
    "with tf.name_scope('phase2'):\n",
    "    phase2_reconstruction_loss=tf.reduce_mean(tf.square(hidden3-hidden1))\n",
    "    phase2_reg_loss=regularizer(weights2)+regularizer(weights3)\n",
    "    phase2_loss=phase2_reconstruction_loss+phase2_reg_loss\n",
    "    train_vars=[weights2,biases2,weights3,biases3]\n",
    "    phase2_training_op=optimizer.minimize(phase2_loss,var_list=train_vars)#提供训练变量\n",
    "    #不包括weights1,biases1,从而冻结第一层\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可视化重建结果\n",
    "n_test_digits=2\n",
    "X_test=mnist.test.image[:n_test_digits]\n",
    "with tf.Session() as sess:\n",
    "    [...]#训练自编码器\n",
    "    outputs_val=outputs.eval(feed_dict={X:X_test})\n",
    "def plot_image(image,shape=[28,28]):\n",
    "    plt.show(image.reshape(shape),cmap='Greys',interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "for digit_index in range(n_test_digits):\n",
    "    plt.subplot(n_test_digits,2,digit_index*2+1)\n",
    "    plt_image(X_test[digit_index])\n",
    "    plt.subplot(n_test_digits,2,digit_index*2+2)\n",
    "    plot_image(outputs_val[digit_index])\n",
    "        \n",
    "        \n",
    "#可视化特征\n",
    "##针对每个隐层的每个神经元，找出最能刺激训练样本的样例，这对高层隐层非常有用。\n",
    "with tf.Session() as sess:\n",
    "    [...]#训练自编码器\n",
    "    weights1_val=weights1.eval()\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plot_image(weights1_val.T[i])\n",
    "    \n",
    "#另一种方法是给自编码器提供一个随机输入图像，测试您感兴趣的神经元的激活程度，\n",
    "#然后执行反向传播来调整图像，使神经元激活得更充分。 如果迭代数次（执行渐变上升），\n",
    "#图像将逐渐变成最令人满意的图像（用于神经元）。 这是一种用于可视化神经元的技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用堆栈自编码网络进行无监督预训练\n",
    "##迁移学习的优势在于，你的神经网络不必学习所有底层特征，只需复用已存在的底层检测器。同样地\n",
    "##如果你拥有大量的无标记样本，你可以先训练一个堆栈自编码网络，然后复用其低层网络，用以创建\n",
    "##符合任务需要的神经网络，再用有标记样本继续训练。\n",
    "<img width='500' src='https://raw.githubusercontent.com/hzg0601/python/master/unsupervised_pretraining_2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过完备自编码（输出比输入大）\n",
    "## 降噪自编码\n",
    "对有噪音的输入进行去噪恢复，防止自编码器简单复制输入。噪音可以是高斯噪音，也可以是输入的随机变换，如dropout。\n",
    "<img width='500' src='https://raw.githubusercontent.com/hzg0601/python/master/denoising_autoencoder.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random noise\n",
    "X=tf.placeholder(tf.float32,shape=[None,n_inputs])\n",
    "X_noisy=X+tf.random_normal(tf.shape(X))#用tf.get_shape会报错，因为返回的是[None,n_inputs]\n",
    "[...]\n",
    "hidden1=activation(tf.matmul(X_noisy,weights)+biases1)\n",
    "[...]\n",
    "reconstruction_loss=tf.reduce_mean(tf.square(outputs-X))\n",
    "[...]\n",
    "\n",
    "\n",
    "\n",
    "#dropout\n",
    "from tensorflow.contrib.layers import dropout\n",
    "keep_prob=0.7\n",
    "is_training=tf.placeholder_with_default(False,shape=(),name='is_training')\n",
    "X=tf.placeholder(tf.float32,shape=[None,n_inputs])\n",
    "X_drop=dropout(X,keep_prob,is_training=is_training)\n",
    "[....]\n",
    "hidden1=ativation(tf.matmul(X_drop,weights1)+biases1)\n",
    "[...]\n",
    "reconstruction_loss=tf.reduce_mean(tf.square(output-X))\n",
    "[...]\n",
    "#在训练阶段将is_training设为True\n",
    "sess.run(training_op,feed_dict={X:X_batch,is_training:True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 稀疏自编码\n",
    "在损失函数中添加正则项，使激活神经元的个数大幅减少。为获得稀疏表示，我们必须首先定义每次训练的稀疏性：通过计算全部训练批次中每个神经元的的平均激活次数，批的大小不能太小。一旦获得每个神经元的平均激活次数，就要为过度活跃的神经元增加稀疏损失项，通常是Kullback-Leibler散度，因为它的梯度表现更好。对任意两个随机分布P和Q，K-L散度定义为：\n",
    "$$D_{KL}(P||Q)=\\sum{i}{P(i)log\\frac{P(i)}{Q(i)}}$$\n",
    "对于稀疏自编码，我们希望测度的是目标概率p和真实概率q之间的散度，可表示为\n",
    "$$D_{KL}(p||q)=p\\ log\\frac{p}{q}+(1-p)log\\frac{1-p}{1-q}$$\n",
    "为区分重建损失和稀疏损失的相对重要性，可以在稀疏损失项上加入稀疏权重超参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p,q):\n",
    "    return p*tf.log(p/q)+(1-p)*tf.log((1-p)/(1-q))\n",
    "learning_rate=0.01\n",
    "sparsity_target=0.1\n",
    "sparsity_weight=0.2\n",
    "\n",
    "[....]\n",
    "\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "hidden1_mean=tf.reduce_mean(hidden1,axis=0)\n",
    "sparsity_loss=tf.reduce_sum(kl_divergence(sparsity_target,hidden1_mean))\n",
    "reconstruction_loss=tf.reduce_mean(tf.square(output-X))\n",
    "loss=reconstruction_loss+sparsity_loss*sparsity_weight\n",
    "training_op=optimizer.minimize(training_op)\n",
    "#编码层的activation必须介于0-1，否则KL会返回NaN,解决该冲突的方法之一是在建模时引入logistic\n",
    "##激活函数\n",
    "hidden1=tf.nn.sgmoid(tf.matmul(X,weights1)+biases)\n",
    "#为加速训练，可以将损失函数由MSE替换为cross entropy,其梯度表现更好，但需要将输入标准化，\n",
    "##输出也要调整为sigmoid函数\n",
    "[...]\n",
    "logits=tf.matmul(hidden1,weights2)+biases2\n",
    "outputs=tf.nn.sigmoid(logits)\n",
    "reconstruction_loss=tf.reduce_sum(\n",
    "tf.nn.sigmoid_cross_entropy_with_logits(labels=X,logits=logits))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变分自编码（variational autoencoders）\n",
    "是概率自编码，因为他们的输出有随机的成分；也是生成自编码，即可以生成从原始样本集中抽取的数据。这两个特征使得他们与RBM非常像,只是训练更容易，抽样也更快。\n",
    "<img width='500' src='https://raw.githubusercontent.com/hzg0601/python/master/variational_autoencoder.png'>\n",
    "\n",
    "\n",
    "隐层1,2构成编码器，隐层4，5构成解码器，与其他自编码器不同的是，第三层并非直接产生编码结果，而是输出编码均值$\\mu$和标准差$\\sigma$,真是编码是从均值为$\\mu$，标准差$\\sigma$得的的高斯分布随机数，而后解码器正常解码。尽管输入的分布非常复杂，变分自编码器仍可以输出与输入类似的样本。在训练阶段，损失函数迫使编码器遍历编码空间（潜空间）生成类似于高斯点构成的球星空间，从而可以得到新样本。<br/>\n",
    "其损失函数由两部分构成：第一部分是重建损失，第二部分是潜在损失（latent loss）迫使自编码器编码输入是类似于高斯分布的抽样，因此用KL散度测度目标分布即高斯分布与分布之间的差异。由于高斯噪音，限制了可转移至编码层的信息量，因此损失函数会比较复杂。潜在损失的代码如下：\n",
    "```python \n",
    "eps=1e-10#平滑项\n",
    "latent_loss=0.5*tf.reduce_sum(\n",
    "tf.square(hidden3_sigma)+tf.square(hidden3_mean))-1-tf.log(eps+tf.square(hidden3_sigma))\n",
    "```\n",
    "另一种变体训练编码器输出$\\gamma=log(\\sigma^2)$而非$\\sigma$,从而$\\sigma=exp(\\frac{\\gamma}{2})$,这种变化会使编码层更容易不同尺度的$\\sigma$，因此收敛速度更快，代码如下：\n",
    "```python\n",
    "latent_loss=0.5*tf.reduce_sum(tf.exp(hidden3_gamma)+tf.square(hidden3_mean)-1-hidden3_gamma)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs=28*28\n",
    "n_hidden1=500\n",
    "n_hidden2=500\n",
    "n_hideen3=20#编码层\n",
    "n_hidden4=n_hidden2\n",
    "n_hidden5=n_hidden1\n",
    "n_outputs=n_inputs\n",
    "\n",
    "learning_rate=-0.001\n",
    "X=tf.placeholder(tf.float32,shape=[None,n_inputs])\n",
    "with tf.contrib.framework.arg_scope(\n",
    "[fully_connected],activation_fn=tf.nn.elu,\n",
    "    weights_initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    hidden1=fully_connected(X,n_hidden1)\n",
    "    hidden2=fully_connected(hidden1,n_hidden2)\n",
    "    hidden3_mean=fully_connected(hidden2,n_hideen3,activation_fn=None)\n",
    "    hidden3_gamma=fully_connected(hidden2,n_hideen3,activation_fn=None)\n",
    "    hidden3_sigma=tf.exp(0.5*hidden3_gamma)\n",
    "    noise=tf.random_normal(tf.shape(hidden3_sigma),dtype=tf.float32)\n",
    "    hidden3=hidden3_mean+hidden3_sigma*noise\n",
    "    hidden4=fully_connected(hidden3,n_hidden4)\n",
    "    hidden5=fully_connected(hidden4,n_hidden5)\n",
    "    logits=fully_connected(hidden5,n_outputs,activation_fn=None)\n",
    "    outputs=tf.sigmoid(logits)\n",
    "    \n",
    "reconstruction_loss=tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=X,logits=logits))\n",
    "latent_loss=0.5*tf.reduce_sum(tf.exp(hidden3_gamma)+tf.square(hidden3_mean)-1-hidden3_gamma)\n",
    "cost=reconstruction_loss+latent_loss\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op=optimizer.minimize(cost)\n",
    "\n",
    "init=tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-092d2f4a8542>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mcodings_rnd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_digits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_hidden3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0moutputs_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mhidden3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcodings_rnd\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_digits=60\n",
    "n_epochs=50\n",
    "batch_size=150\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "def plot_image(image,shape=[28,28]):\n",
    "    plt.show(image.reshape(shape),cmap='Greys',interpolation='nearest')\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches=mnist.train.num_examples//batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch,y_batch=mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op,feed_dict={X:X_batch})\n",
    "    codings_rnd=np.random.normal(size=[n_digits,n_hidden3])\n",
    "    outputs_val=outputs.eval(feed_dict={hidden3:codings_rnd})\n",
    "for iteraion in range(n_digits):\n",
    "    plt.subplot(n_digits,10,iteration+1)\n",
    "    plot_image(outputs_val[iteration])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其他自编码器\n",
    "## 收缩自编码Contractive autoencoder(CAE)\n",
    "## 堆栈卷积自编码stacked convolutional autoencoder\n",
    "## 生成式随机网络generative stochastic network\n",
    "## winner-take-all 自编码\n",
    "## 对抗式自编码adversarial autoencoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
