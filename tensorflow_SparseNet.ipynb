{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#稀疏性的生物机理\" data-toc-modified-id=\"稀疏性的生物机理-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>稀疏性的生物机理</a></span></li><li><span><a href=\"#稀疏响应与数学物理描述\" data-toc-modified-id=\"稀疏响应与数学物理描述-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>稀疏响应与数学物理描述</a></span></li><li><span><a href=\"#稀疏深度网络模型及基本性质\" data-toc-modified-id=\"稀疏深度网络模型及基本性质-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>稀疏深度网络模型及基本性质</a></span></li><li><span><a href=\"#数据的稀疏性\" data-toc-modified-id=\"数据的稀疏性-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>数据的稀疏性</a></span></li><li><span><a href=\"#稀疏正则\" data-toc-modified-id=\"稀疏正则-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>稀疏正则</a></span></li><li><span><a href=\"#稀疏连接\" data-toc-modified-id=\"稀疏连接-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>稀疏连接</a></span><ul class=\"toc-item\"><li><span><a href=\"#DropOut\" data-toc-modified-id=\"DropOut-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>DropOut</a></span></li><li><span><a href=\"#矩阵分解稀疏连接\" data-toc-modified-id=\"矩阵分解稀疏连接-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>矩阵分解稀疏连接</a></span></li></ul></li><li><span><a href=\"#稀疏分类器设计\" data-toc-modified-id=\"稀疏分类器设计-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>稀疏分类器设计</a></span></li><li><span><a href=\"#softmax分类器\" data-toc-modified-id=\"softmax分类器-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>softmax分类器</a></span></li><li><span><a href=\"#Sparsemax分类器\" data-toc-modified-id=\"Sparsemax分类器-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Sparsemax分类器</a></span></li><li><span><a href=\"#深度学习中关于稀疏的技巧\" data-toc-modified-id=\"深度学习中关于稀疏的技巧-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>深度学习中关于稀疏的技巧</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 稀疏性的生物机理\n",
    "随着层级信息不断的'加深',不同视觉皮层上的神经细胞对特定形状的视觉图案有最佳的响应和偏好刺激,简言之,层级越高感受野越大,即信息处理从局部到更大的区域,类似尺度特性.层级较低时,感受野所处理的趋于较小,稀疏性越强(特指层级之间的连接特性),层级较高时,感受野所处理的区域越大,稀疏性越弱.<br/>\n",
    "当层级较低时,其简单细胞对应着严格的方向和带通特性,而复杂细胞在保持简单细胞特性的基础上进一步具有局部变换(平移)不变性。简言之，简单细胞处理信息具有稀疏性（即局部连接），而复杂细胞具有聚类（连接计算共享）特性。稀疏编码是视觉系统中图像表示的主要方式，初级视觉皮层（V1）中的神经元对视觉信息的反应具有稀疏性，V4区的神经元通过稀疏编码的方式实现视觉信息的表示。<br/>\n",
    "计算神经科学：理解并分析大脑如何在算法层面上工作交叉学科。<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 稀疏响应与数学物理描述\n",
    "常见的稀疏性是指向量中绝大多数元素的值为零或者接近于零，而广义的稀疏性是指通过特定变换后目标的稀疏性。<br/>\n",
    "稀疏编码和稀疏表示：关于系数的稀疏性约束，前者采用光滑可导的函数，而后者采用伪范数或$L_1$范数，且稀疏编码不要求基原子的个数一定要大于数据的维度。\n",
    "### 稀疏深度网络模型及基本性质\n",
    "（显示稀疏）线性稀疏模型：压缩感知，双稀疏模型，结构化稀疏模型（基于正则项的设计如群稀疏，图稀疏，随机场稀疏），S-HMAX为稀疏层级识别模型，SRC稀疏表示分类器。隐式稀疏：它通常内蕴在非线性激活函数和损失函数的构建过程中。<br/>\n",
    "深度神经网络中引入稀疏正则或蕴含稀疏性可以认为是病态模型良态化的过程，如<br/>**稀疏正则的核心是解决过拟合的问题，稀疏权值连接(DropOut)的本质是通过约减参数量间接增加训练数据，非线性激活函数中所隐含的稀疏性是为了增加‘扭曲’程度，即不同类别的（线性不可分）输入随着层级的增加，隐层特征所对应的线性可分性逐渐增加。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据的稀疏性\n",
    "数据的稀疏性包含三点：<br/>一是数据中所包含某种拓扑特性或目标相对数据本身呈现出非零元素较少的情形。<br/>**二是数据在某种(线性或非线性）自适应或非自适应变化下对应的表示系数具有非零元素较少的状况。**<br/>三是随着数据集规模的增加，呈先某种统计或物理特性的数据占整个数据集的少数。<br/>\n",
    "目前，常用的稀疏性描述是基于第二点假设，并且作为一种有效的（稀疏性）正则约束，在优化目标函数关于解存在多样性的问题中给出了合理的解释与逼近。关于第一点，通常可以作为一种有效的处理方式（如二值化处理，或者零化无关区域）关于第三点，其核心问题是如何利用稀疏编码筛选出这些重要的样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 稀疏正则\n",
    "$L_2$范数（Tikhonov Regularization)并没有蕴含稀疏特性，$L_1$范数通常可以诱导出系数特性。<br/>除了在权值连接上引入稀疏正则外，还可以在某个隐层输出层引入稀疏性，例如：$$\\begin{cases}\\underset{\\theta}{min}J(\\theta)=||\\boldsymbol{y}-\\boldsymbol{x}\\theta||_2^2+\\lambda\\cdot R(\\theta)\\\\ \\underset{\\phi}{min}J(\\phi)=||\\boldsymbol{x}-\\boldsymbol{D}\\cdot \\phi||_2^2+\\lambda \\cdot ||\\phi||_1\\end{cases}$$\n",
    "其中，$\\boldsymbol{D}$为字典，数学中称之为框架，即有冗余的“基”；x为输入，**$\\phi$为输出**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 稀疏连接\n",
    "卷积神经网络的特性包括：局部连接，权值共享和平移不变性。这些特性都具有稀疏性。<br/>\n",
    "局部连接相对于全连接，更符合外侧膝状体到初级视觉皮层上的稀疏响应特性。权值共享，进一步约束相似隐单元具有同样的激活特性，使局部连接后的权值具有结构性，实际应用中可以进一步约减参数，间接增加数据量；最后，平移不变性由池化方式诱导获取，也可以认为一种有效‘删除’参数的方式，即带有稀疏性的零化操作。<br/>\n",
    "#### DropOut\n",
    "正常的连接\n",
    "$$\\begin{cases}z_i^{(l+1)}=\\boldsymbol{W}_i^{(l+1)}\\cdot y^{(l)}+b_i^{(l+1)}\\\\y_i^{(l+1)}=\\sigma (z_i^{(l+1)}\\end{cases}$$\n",
    "带DropOut策略的连接\n",
    "$$\\begin{cases}r_j^{(l)}\\tilde{}Benoulli(p)\\\\ \n",
    "\\tilde{\\boldsymbol{y}}^{(l)}=\\boldsymbol{r}^{(l)}\\bigodot \\boldsymbol{y}^{(l)}\\in\\mathbb{R}^3\\\\\n",
    "z_i^{(l+1)}=\\boldsymbol{W}_i^{(l+1)}\\cdot \\tilde{\\boldsymbol{y}}^{(l)}+b_i^{(l+1)}\\\\\n",
    "y_i^{(l+1)}=\\sigma(\\boldsymbol{z}^{(l+1)}_i)\\end{cases}$$\n",
    "其中$\\bigodot$代表对应元素相乘，$p$为相应的概率即DropOut率，应用中，隐含节点DropOut率等于0.5的时候效果最好，主要原因在于此时DropOut随机生成的网络结构最多。<br/>\n",
    "另一种稀疏连接可以通过约减参数的方式来实现，通常有两种思路：一种是直接将小的权值连接置为零(但有风险，因为随着层级的上升，较小的权值将会使得输入累积较为大的输出）；二是通过矩阵分解来实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵分解稀疏连接\n",
    "假设输入与输出的关系为：$$\\begin{cases}\\boldsymbol{y}=\\sigma(\\boldsymbol{W\\cdot x}+b)\\\\\\boldsymbol{W}\\in\\mathbb{R}^{n\\times m}\\end{cases}$$\n",
    "假设$rank(\\boldsymbol{W})=r$，且$\\boldsymbol{W}$可以通过矩阵奇异值分解得到$\\boldsymbol{W=U\\cdot \\Sigma \\cdot V^T}$，则通过组合策略得到的权值连接表示为：$$\\begin{cases}\\boldsymbol{W=W_1\\cdot W_2}\\\\\\boldsymbol{W}_2\\in\\mathbb{R}^{n\\times r}\\\\\\boldsymbol{W}_1\\in\\mathbb{R}^{r\\times m}\\end{cases}$$模型相对应变为：$$\\begin{cases}\\boldsymbol{z=W_1\\cdot x}\\\\\\boldsymbol{y}=\\sigma(\\boldsymbol{W_2\\cdot z}+b)\\end{cases}$$由此，参数由$n\\cdot m$变为$r\\cdot(n+m)$,该规则有效的前提是权值连接$\\boldsymbol{W}$是低秩矩阵，即$rank(\\boldsymbol{W})=min(n,m)$由于在大多数情况下，权值矩阵是满秩的，因此通常取$\\Sigma$较大的k个奇异值并将其它奇异值置为零，来实现对$\\boldsymbol{W}$的逼近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 稀疏分类器设计\n",
    "常见的稀疏分类器设计是基于表示学习的，如稀疏分类器，其核心步骤包括:<br/>(1)字典构造$$\\boldsymbol{D=[D_1,...,D_K]}$$，其中K为类别个数，$\\boldsymbol{D}_k$为第k类样本或数据集构造（直接将样本堆叠形成）或学习（通过K-SVD)的字典<br/>\n",
    "（2）对于样本$\\boldsymbol{x}$进行如下的表示学习：$$\\underset{\\alpha}{min}\\frac{1}{2}\\cdot||\\boldsymbol{x-D\\cdot \\alpha}||_2^2+\\lambda\\cdot||\\boldsymbol{\\alpha}||_1$$其中，$\\boldsymbol{\\alpha=[\\alpha_1,...,\\alpha_K]}^T$.基于假设，若样本$\\boldsymbol{x}$属于第k类，则表示系数主要集中在$\\boldsymbol{\\alpha}_k$，而其它系数$\\boldsymbol{\\alpha}_j(j\\ne k)$期望为零，这里的$\\boldsymbol{\\alpha}_k$是向量，而不是标量。$$label(\\boldsymbol{x})=arg\\underset{1\\leq k\\leq K}{min}=\\{||\\boldsymbol{x-D_k\\cdot \\alpha_k}||\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax分类器\n",
    "$$\\begin{cases}\\boldsymbol{y}=Softmax(\\boldsymbol{x},\\theta)=[y_1,...,y_K]^T\\\\y_k=P(label(\\boldsymbol{x})=k|\\boldsymbol{x},\\theta_k)=\\frac{1}{\\sum_{j=1}^K{e^{(\\boldsymbol{x},\\theta_j)}}}\\cdot e^{(\\boldsymbol{x},\\theta_j)}\\end{cases}$$\n",
    "另一种系数分类器的设计是基于改进的Softmax分类器来实现的，其动机是改进Softmax输出处处不为零以期获得输出大多数为零，并记此为Sparsemax分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsemax分类器\n",
    "$$\\begin{cases}\\boldsymbol{y}=Sparemax(\\boldsymbol{x},\\phi)=arg\\underset{\\boldsymbol{p}\\in\\Delta_{(k-1)}}{min}||\\boldsymbol{p}-(\\boldsymbol{W\\cdot x}+b)||_2^2\\\\\\phi=(\\boldsymbol{W},b)\\end{cases}$$其中，$\\Delta^{(k-1)}$为单形，即满足：$$\\Delta^{(k-1)}=\\left\\{\\boldsymbol{p}\\in\\mathbb{R}^K:\\sum_{i=1}^K{p(i)=1},p(i)\\ge 0\\right\\}$$若已知参数$\\phi$和输入$\\boldsymbol{x}$,记$\\boldsymbol{z}=\\boldsymbol{W\\cdot x}+b$,关于$\\boldsymbol{p}\\in\\Delta^{k-1}$有如下形式的解：$$p_k=[z_k-\\tau(\\boldsymbol{z})]_+$$\n",
    "其中$p_k$是将输入$\\boldsymbol{x}$分到第k类的概率，其中$k=1,2,...,K$和$[t]_+=max(0,t)$,$\\sum_k{[z_k-\\tau(\\boldsymbol{z})]_+}=1$<br/>\n",
    "记$\\boldsymbol{z}$的坐标排序为:$z_1\\ge z_2\\ge...\\ge z_K$,并定义$k(\\boldsymbol{z})=max\\left\\{k\\in [1,2,...,K]:1+k\\cdot z_k\\ge \\sum_{j\\ge k}{z_j}\\right\\}$,则$\\tau(\\cdot)$可以表示为：$$\\tau(\\boldsymbol{z})=\\frac{\\left(\\sum_{j\\leq k(\\boldsymbol{z})}{z_j}\\right)-1}{k(\\boldsymbol{z})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度学习中关于稀疏的技巧\n",
    "稀疏深度网络模型的设计包含以下三条准则：<br/>\n",
    "(1)层级间模块化，逐层堆栈。<br/>\n",
    "自编码网络对应深度堆栈网络（隐节点输出的稀疏正则性约束包括KL散度和L1范数或伪范数），稀疏受限boltzman机对应的深度置信网络，卷积稀疏编码对应的(稀疏）反卷积神经网络。<br/>\n",
    "(2)逐阶段模块化。<br/>\n",
    "与层级间模块化不同，针对特定任务，如分类，利用生成对抗网络（包括两个子网络，生成模型和判别式模型）在无监督学习方式下获取非合作状态下的零和博弈解，提取其判别网络中的特征学习部分（去掉后面的真伪二值分类器），结合分类器设计，再利用监督方式进行整个网络（由提取的特征学习部分和分类器设计部分组合而成）的精调，其中稀疏化可以内蕴在特征学习部分的分类器中。<br/>\n",
    "(3)多通路网络设计。<br/>\n",
    "多分辨特性可以认为是输入在不同尺度或不同频带上的响应，相比较单尺度上对输入的（稠密性）表征，多分辨特性通过多通路或多通道来散化对输入的表征，使其在每一个尺度上或频带上呈现稀疏性。另外，根据深度神经网络的设计准则，塔式、对称和多通路可以削弱‘深度’对输入和输出间的非线性刻画，即极深神经网络（例如深度残差网络，深度分形网络）可以由多通路，带有融合特性的深度神经网络来逼近。<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在以上三条准则的基础上，常用的稀疏策略包括DropOut,DropConnect,DropNeuron等；而网络中激活函数的有效选择，将有助于通过内蕴稀疏性来提升网络的泛化性能和计算开销，常用的激活函数包括,ReLU,RePLU,Maxout(本质上ReLU是Maxout的一种特例,Maxout对若干隐隐层单元的值执行最大化操作,而ReLU执行与0比较操作）。<br/>\n",
    "深度学习是基于数据的,能否通过对数据的'分级处理',如聚类,通过划定与聚类中心的亲疏来实现样本的分级处理,进一步,对每级样本分别来学习深度网络,以期探索数据的差异性对深度卷积神经网络的影响.如大脑的多分辨特性,对完整信息(分辨率高)的输入识别精度高,反之则低.若将这种多分辨性与深度卷积网络相结合,形成多分辨深度卷积神经网络,实现对样本的筛选并改善基于差异性数据集学习到的深度卷积神经网络的性能.<br/>\n",
    "当然,过分地强调稀疏性处理,会减少模型的有效容量,即特征屏蔽太多,理想的稀疏性比率保持在70%~85%<br/>\n",
    "从实验中可以看出ReLU激活函数隐含对特征'非负稀疏性'的要求;而(最大)池化操作隐含对特征'强稀疏性'(特征选择)的要求;以及参数层偏置隐含对特征'稀疏度'的调节."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
