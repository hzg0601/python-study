{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensionality\n",
    "##加速训练；方便可视化\n",
    "##维度越高，任意两点的距离越远。类似地，维度越高，过拟合风险越大。\n",
    "\n",
    "#映射\n",
    "##在现实中，样本并不是随特征均匀分布，可能一些特征是常数，另一些特征高度相关，这种情况下，\n",
    "###就可以将高位特征映射到低位子空间。\n",
    "\n",
    "#流形学习\n",
    "\n",
    "##当原始的数据的流形不是线性的，则简单的映射并不能取得好的降维效果\n",
    "##流形假设：多数高维数据可以嵌入到一个低维流形中。预测任务在低位空间流形上表示更简单。\n",
    "\n",
    "#PCA\n",
    "##第一步找到与数据最接近的超平面，然后将数据映射到超平面上。\n",
    "##寻找保留最大方差（最小平方和）的映射，从而可以保留最多的信息。\n",
    "##PCA方法会依次找到保留最大方差、第二、第三方法的低维平面。\n",
    "##PCA主成分的方向并不固定。\n",
    "##PCA的主要方法是SVD，X=U*Σ*V^T,其中V^T包含了所有的主成分。\n",
    "#X_d-proj=X*W_d需要进行中心化\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from scipy.io import loadmat\n",
    "\n",
    "mnist_raw = loadmat('C:\\\\Users\\\\hzg0601\\\\datasets\\\\mnist-original.mat')\n",
    "mnist = {\n",
    "        \"data\": mnist_raw[\"data\"].T,\n",
    "        \"target\": mnist_raw[\"label\"][0],\n",
    "        \"COL_NAMES\": [\"label\", \"data\"],\n",
    "        \"DESCR\": \"mldata.org dataset: mnist-original\",\n",
    "    }\n",
    "\n",
    "#fetch_mldata?\n",
    "#mnist = fetch_mldata('MNIST original')\n",
    "X,y=mnist['data'],mnist['target']\n",
    "\n",
    "X_centered=X-X.mean(axis=0)\n",
    "U,s,V=np.linalg.svd(X_centered)\n",
    "X_2D=X_centered.dot(V.T[:,:2])\n",
    "\n",
    "#按数目选取\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_component2=2)\n",
    "x2D=pca.fit_transform(X)\n",
    "pca.conponents_.T[:,0]#主成分\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "#按方差贡献率选择主成分个数\n",
    "pca=PCA(n_components=0.95)\n",
    "X_reduced=pca.fit_transform(X)\n",
    "\n",
    "#也可以以cumsum按方差贡献画出Dimensions-Explained Variance图，在拐点处选择维度作为降维的标准。\n",
    "\n",
    "#pca方法可以调用inverse_transform()将数据恢复\n",
    "pca=PCA(n_components=154)\n",
    "X_reduced=pca.fit_transform(X)\n",
    "X_recovered=pca.inverse_transform(X_reduced)\n",
    "\n",
    "\n",
    "#Incremental PCA支持在线学习，将整个数据集拆分成mini-batched输入IPCA算法\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "n_batches=100\n",
    "inc_pca=IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X,n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "X_reduced=inc_pca.transform(X)\n",
    "\n",
    "#调用np.memmap类进行增量PCA\n",
    "\n",
    "import numpy as np\n",
    "X_mm=np.memmap(X,dtype='float32',mode='readonly',shape=(m,n))\n",
    "batch_size=m//n_batches\n",
    "inc_pca=IncrementalPCA(n_components=154,batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)\n",
    "\n",
    "\n",
    "##Randomized PCA\n",
    "###随机PCA利用随机算法期望快速找到d主成分的近似。它的时间复杂度O(m*d^2)+O(d^3),而不是\n",
    "###O(m*d^2)+O(d^3)\n",
    "rnd_pca=PCA(n_components=154,svd_solver='randomized')\n",
    "X_reduced=rnd_pca.fit_transform(X)\n",
    "\n",
    "\n",
    "##Kernel PCA\n",
    "##KPCA可以进行非线性降维，能够保持映射后的聚类特性，甚至能将依附于非线性流形上的数据降维展开。\n",
    "from sklearn.decomposition import KernelPCA\n",
    "rbf_pca=KernelPCA(n_components=2,kernel='rbf',gamma=0.04)\n",
    "X_reduced=rbf_pca.fit_transform(X)\n",
    "\n",
    "\n",
    "#选择合适的Kernel与调参\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from slearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf=Pipeline([\n",
    "    ('kpca',KernelPCA(n_components=2)),\n",
    "    ('log_reg',LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid=[{\n",
    "    'kpca__gamma':np.linspace(0.03,0.05,10),\n",
    "    'kpaca__kernel':['rbf','sigmoid']\n",
    "}]\n",
    "\n",
    "grid_serch=GridSearchCV(clf,param_grid,cv=3)\n",
    "gird_search.fit(X,y)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "##无监督方法：寻找最小重建误差的kernel和超参数\n",
    "##降维后，将数据映射会特征空间，找到原始数据与映射数据的对应点，计算二者的距离，\n",
    "#即pre-image误差、\n",
    "#pre-image误差\n",
    "rbf_pca=KernelPCA(n_components=2,kernel='rbf',gamma=0.0433,fit_inverse_transform=True)\n",
    "X_reduced=rbf_pca.fit_transform(X)\n",
    "X_preimage=rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "mse(X,X_preimage)\n",
    "\n",
    "##局部线性嵌入（LLE)\n",
    "##LLE是一种流形学习方法，但不依赖于映射。\n",
    "##LLE首先测度每个样本与近邻样本的距离，而后寻找能将近邻关系保持的低维表示。这种方法在\n",
    "##噪音较少的数据中非常有效。\n",
    "from sklearn.manifold import LocallyLinearEmbedding as LLE\n",
    "lle=LLE(n_components=2,n_neighbors=10)\n",
    "X_reduced=lle.fit_transform(X)\n",
    "\n",
    "#1,针对每个样本，找到其最近的k个近邻样本。找到样本权重\n",
    "#2，将该样本用其k个近邻样本以现行函数进行重构。对样本权重进行正则化。\n",
    "\n",
    "##其他方法\n",
    "###Multidimensional Scaling(MDS)\n",
    "###Isomap\n",
    "###t-Distributed Stochastic Neighbor Embedding\n",
    "###Linear Discriminant Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
