{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs=28*28\n",
    "n_hidden1=300\n",
    "n_hidden2=100\n",
    "n_outputs=10\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "#1.梯度消失或爆炸；难于训练；容易过拟合\n",
    "\n",
    "#梯度爆炸易发生于RNN\n",
    "#在sigmold函数和正态随机初始化方法下，每层输出的方差要远远大于输入的方差。在向前传播的过程\n",
    "#中，方差持续增大，而logisitc函数的均值为0.5。输入越大，logistic的梯度越小。因此在最高层，\n",
    "#梯度已然非常小，在反向传播过程中越来越小直至消失。解决的办法是让输出的方差与输入的方差相等\n",
    "#梯度反向传播过程中的方差也不变，Xavier/Glorot initialization方法可在一定程度上实现这种可能。\n",
    "#ReLU 与初始化策略相结合称为He initialization  P.278\n",
    "\n",
    "\n",
    "#fully_connected()默认为Xavier（均匀分布）初始化。\n",
    "he_init=tf.contrib.layers.variance_scaling_initializer()#He初始化，可更改mode=\"FAN-AVG\"\n",
    "hidden1=fully_connected(X,n_hidden1,weights_initializer=he_init,scope='h1')\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")\n",
    "##书中使用tensorflow.contrib.layers.fully_connected()而不是tf.layers.dense()（本章编写时不存在）。 \n",
    "##现在最好使用tf.layers.dense()，因为contrib模块中的任何内容可能会更改或删除\n",
    "#dense()函数几乎与fully_connected()函数完全相同。 与本章有关的主要差异是：\n",
    "###scope变成name，activation_fn变成activation（类似地，_fn后缀从诸如normalizer_fn之类的其他参数中移除），\n",
    "###weights_initializer变成kernel_initializer等等。默认activation现在是None，而不是tf.nn.relu。 \n",
    "###它不支持tensorflow.contrib.framework.arg_scope(),它不支持正则化的参数（稍后在第 11 章介绍）。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fan-in /fan-out输入/输出连接的数目，He初始化只考虑fan-in，并不是Xavier那样的的fan-in\n",
    "#和fan-out的均值。\n",
    "\n",
    "#非饱和激活函数\n",
    "#elu()\n",
    "hidden1=fully_connected(X,n_hidden1,activation_fn=tf.nn.elu)\n",
    "\n",
    "#leaky ReLU\n",
    "def leaky_relu(z,name=None):\n",
    "    return tf.maximum(0.01*z,z,name=name)\n",
    "hidden1=fully_connected(X,n_hidden1,activation_fn=leaky_relu)\n",
    "\n",
    "#批正则化 batch normalization\n",
    "\n",
    "###用以解决梯度消失或爆炸，因为在训练过程中，由于前一层的参数发送变化，每层的输入的分布\n",
    "#会变化变化，称为内部协变量漂移internal covariate shift\n",
    "#在每层的激活函数前增加输入的标准化操作，增加尺度和漂移参数去标准化结果。即该操作迫使模型\n",
    "#学习每层的最优规模和输入均值。该操作要求每次都要计算mini-batch的均值和标准差\n",
    " #P.282\n",
    "#在测试阶段，则用整个训练集去计算均值和标准差，在训练阶段则用移动平均方法。因此每个批正则\n",
    "#化层需要进行4个参数的学习：γscale,βoffset,μmean,σstandard deviation.\n",
    "\n",
    "#批正则化可以大幅缓解梯度消失或爆炸问题，甚至可以用饱和激活函数，对初始化权值也不那么敏感，\n",
    "#甚至可以设较大的学习率，同时降低对其他正则化技术的需求。\n",
    "\n",
    "#批正则化省去了输入数据的正则化，但仍增加了模型的复杂度。因此，从计算速度角度考虑，需权衡\n",
    "#ELU+He初始化与批正则化的表现差异。\n",
    "\n",
    "\n",
    "is_training=tf.placeholder(tf.bool,shape=(),name='is_training')\n",
    "#是否在训练阶段；如是进行mini-batch正则化；否则对全测试集正则化。\n",
    "bn_params={'is_training':is_training,'decay':0.99,'updates_collections':None}\n",
    "#正则化的参数；算法以指数递减的方法计算运行平均running averages;即学习率的衰减参数\n",
    "#updates_collections控制是否在批正则化前更新running averages。\n",
    "hidden1=fully_connected(X,n_hidden1,scope='hidden1',normalizer_fn=batch_norm,\n",
    "                       normalizer_params=bn_params)\n",
    "hidden2=fully_connected(hidden1,n_hidden2,scope='hidden2',normalizer_fn=batch_norm,\n",
    "                       normalizer_params=bn_params)\n",
    "logits=fully_connected(hidden2,n_outputs,activation_fn=None,scope='output',\n",
    "                      normalizer_fn=batch_norm,normalizer_params=bn_params)\n",
    "#ReLU函数无需进行scale;但其他激活函数需在bn_params设定scale=True\n",
    "\n",
    "#条件域简化参数\n",
    "with tf.contrib.framework.arg_scope(\n",
    "[fully_connected],\n",
    "normalizer_fn=batch_norm,\n",
    "normalizer_params=bn_params):\n",
    "    hidden1=fully_connected(X,n_hidden1,scope='hidden12')\n",
    "    hidden2=fully_connected(hidden1,n_hidden2,scope='hidden22')\n",
    "    logits=fully_connected(hidden2,n_outputs,scope='outputs2',activation_fn=None)\n",
    "    \n",
    "#梯度修剪\n",
    "##进行梯度修剪，使梯度值不超过阈值以防止梯度爆炸。梯度修剪已为批正则化替代。\n",
    "##优化器的minimize函数会先计算随后应用梯度，因此为应用梯度修剪需先调用cimpute_gradients()\n",
    "##然后调用clip_by_value函数修剪梯度，最后调用apply_gradients\n",
    "threshold=1.0\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars=optimizer.compute_gradients(loss)\n",
    "capped_gvs=[(tf.clip_by_value(grad,-threshold,threshold),var) for grad,var in grads_and_vars]\n",
    "training_op=optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "#复用预训练层\n",
    "##利用低层的网络参数进行迁移学习，而不是从头开始训练。输入特征相似的数据才能进行迁移学习。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非饱和激活函数\n",
    "###ReLU的良好表现来源于其对正值的非饱和性。但并非完美\n",
    "#dying ReLU:在训练中，除了0之外没有输出。有时候会出现一半的ReLU失效，尤其是当学习率设的\n",
    "比较高的时候。当神经元权值更新，输入的加权和小于0时，神经元只能输出0.\n",
    "\n",
    "### leaky ReLU\n",
    "漏斗ReLU_{\\alpha}=max{\\alpha z,z},alpha通常设为0.01，长久休眠，但有机会苏醒。\n",
    "#leaky ReLU的表现比ReLU好，并且alpha较大比较小好。\n",
    "\n",
    "### random leaky ReLU\n",
    ",alpha是在训练时随机挑选，在测试时固定在平均水平。\n",
    "\n",
    "### parametric leaky ReLU，\n",
    "alpha变成需要训练的参数。在大数据中表现优异，但在小数据中容易过拟合。\n",
    "\n",
    "### exponetional linear unit (ELU) \n",
    "$ELU_{\\alpha}(z)=\\begin{cases}\\alpha(exp(z)-1)&\\ if\\ z<0\\\\z&if\\ z\\ge 0\\end{cases}$<br/>\n",
    "在小于0处为负，保证输出均值接近0；在小于0时，梯度不为0；处处平滑，可加速梯度下降。但计算要更慢，训练也更慢。<br/>\n",
    "一般选择：ELU>leaky ReLU>ReLU>tanh>logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在完成模型设计后，运行模型的时机由batch_norm层控制，需设定is_training占位符True或False\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch,y_batch in zip(X_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={is_training:True,X:batch,y:y_batch})\n",
    "        accracy_score=accuracy.eval(\n",
    "        feed_dict={is_training:False,X:X_test_scaled,y:y_test})\n",
    "        print(accuracy_score)\n",
    "\n",
    "#复用TensorFlow模型\n",
    "##复用全部模型\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./my_original_model.ckpt')\n",
    "##仅复用一些变量的子集,如只复用hidden1,2,3\n",
    "init=tf.global_variables_initializer()\n",
    "reuse_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='hidden[123]')\n",
    "reuse_vars_dict=dict([(var.name,var.name) for var in reuse_vars])\n",
    "original_saver=tf.Saver(reuse_vars_dict)#saver只存储与原模型对应的部分变量\n",
    "new_saver=tf.Saver()#存储新模型\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    original_saver.restore('./my_orignal_model.ckpt')#从原模型重用hidden1,2,3\n",
    "    '''....'''#完成训练\n",
    "    new.saver.save('./my_new_model.ckpt')#存储新模型\n",
    "    \n",
    "    \n",
    "#复用其他框架的模型\n",
    "##如果模型是以其他框架写成的（如Theano），则只能手动指定\n",
    "original_w=[...]#从其他框架加载模型参数\n",
    "original_b=[...]#\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_inputs),name=\"X\")\n",
    "hidden1=fully_connected(X,n_hidden1,scope='hidden1')\n",
    "[...]#创建模型的其他部分\n",
    "#处理fully_connected创建的变量\n",
    "with tf.variable.scope('',default_name='',reuse=True):#根命名域\n",
    "    hidden1_weights=tf.get_variable('hidden1/weights')\n",
    "    hidden1_biases=tf.get_variable('hidden1/biases')\n",
    "    \n",
    "#创建节点并初始化\n",
    "original_weights=tf.placeholder(tf.float32,shape=(n_inputs,n_hidden1))\n",
    "original_biases=tf.placeholder(tf.float32,shape=(n_hidden1))\n",
    "assign_hidden1_weights=tf.assign(hidden1_weights,original_weights)\n",
    "assign_hidden1_biases=tf.assign(hidden1_biases,original_biases)\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights,feed_dict={original_weights:original_w})\n",
    "    sess.run(assign_hidden1_biases,feed_dict={original_biases:original_b})\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#封存（freezing） 低层网络\n",
    "##底层网络用于获得底层特征，而底层特征是训练的基础，因此复用底层网络非常有用，当底层网络的\n",
    "##权重确定，高层网络的训练会非常容易进行。封存底层网络最好的办法就是将要训练的但不包括底层\n",
    "#网络的变量传递给优化器：\n",
    "train_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='hidden[34]outputs')\n",
    "training_op=optimizer.minimize(loss,var_list=train_vars)#1,2层称为封存层\n",
    "\n",
    "\n",
    "#缓存封存层\n",
    "##由于封存层的权重不会改变，因此对于每个训练样本，可以缓存出最高的封存层的输出，训练过程要\n",
    "##遍历整个样本很多次，而缓存封存层的方法可以大大提高训练速度，因为对于每个样本只需遍历一次。\n",
    "\n",
    "hidden2_outputs=sess.run(hidden2,feed_dict={X:X_train})#用整个样本集训练底层网络\n",
    "\n",
    "import numpy as np\n",
    "n_epochs=100\n",
    "n_batches=500\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_idx=rnd.permutation(len(hidden2_ouputs))\n",
    "    hidden2_batches=np.array_split(hidden2_outputs[shuffled_idx],n_batches)\n",
    "    #不在用训练样例的batch,而是底层网络输出的batch\n",
    "    y_batches=np.array_split(y_train[shuffled_idx],n_batches)\n",
    "    for hidden2_batch,y_batch in zip(hidden2_batches,y_batches):\n",
    "        sess.run(training_op,feed_dict={hidden2:hidden2_batch,y:y_batch})\n",
    "    \n",
    "#更快的优化器\n",
    "\n",
    "##Momentum优化器\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "\n",
    "##NMG/NAG\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9,\n",
    "                                     use_nesterov=True)\n",
    "##AdaGrad\n",
    "\n",
    "##RMSProp\n",
    "optimizer=tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                    momentum=0.9,decay=0.9,epsilon=1e-10)\n",
    "##Adam\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "##FTRL系数模型优化\n",
    "FTRLOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "##学习率调整\n",
    "\n",
    "initial_learning_rate=0.1\n",
    "dacay_steps=10000\n",
    "dacay_rate=1/10\n",
    "global_step=tf.Variable(0,trainable=False)\n",
    "learning_rate=tf.train.exponential_decay(initial_learning_rate,global_step,dacay_steps,\n",
    "                                        dacay_rate)\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate,momentum=0.9)\n",
    "training_op=optimizer.minimize(loss,global_step=global_step)\n",
    "\n",
    "#其中，global_step用于记录当前迭代次数。然后调用exponential_decay函数并调用指数衰减学习率\n",
    "#eta_0=0.1,r=10000,然后创建优化器MomentumOptimizer，调用衰减学习率。\n",
    "##AdaGrad,RMSProp,Adam优化器是自适应学习率算法，不需要进行学习率调整。对其他优化器，指数\n",
    "#和表现调整可以很快收敛。\n",
    "\n",
    "#正则化\n",
    "\n",
    "##L_1和L_2正则化\n",
    "base_loss=tf.reduce_mean(xentropy,name='avg_xentropy')\n",
    "reg_losses=tf.reduce_sum(tf.abs(weights1))+tf.reduce_sum(tf.abs(weights2))\n",
    "loss=tf.add(base_loss,scale*reg_losses,name='loss')\n",
    "#当网络层数较多时，这种方法不是很方便。Tensorflow中再创建变量时提供了*_regularizer选项，\n",
    "##如l1_regularizer(),l2_regularizer,l1_l2_regularizer():\n",
    "with arg_scope([fully_connected],\n",
    "    weights_regularizer=tf.contrib.layers.l1_regularizer(scale=0.01)):\n",
    "    hidden1=fully_connected(X,n_hidden1,scope='hidden1')\n",
    "    hidden2=fully_connected(hidden1,n_hidden2,scope='hidden2')\n",
    "    logits=fully_connected(hidden2,n_outputs,activation_fn=None,scope='out')\n",
    "\n",
    "    \n",
    "reg_loss=tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss=tf.add_n([base_loss]+reg_losses,name='loss')\n",
    "\n",
    "\n",
    "#Dropout\n",
    "#在训练阶段，随机地让一些隐层失活。\n",
    "#提高dropout_rate可以防止过拟合\n",
    "from tensorflow.contrib.layers import dropout\n",
    "[...]\n",
    "is_training=tf.placeholder(tf.bool,shape=(),name='is_training')\n",
    "keep_prob=0.5\n",
    "X_drop=dropout(X,keep_prob,is_training=is_training)\n",
    "hidden1=fully_connected(X_drop,n_hidden1,scope='hidden1')\n",
    "hidden1_drop=dropout(hidden1,keep_prob,is_training=is_training)\n",
    "hidden2=fully_connected(hidden1_1,n_hidden2,scope='hidden2')\n",
    "hidden2_drop=dropout(hidden2,keep_prob,is_training=is_training)\n",
    "logits=fully_connected(hidden2_drop,n_outputs,activation_fn=None,scope='outputs')\n",
    "\n",
    "#Max-Norm正则化\n",
    "##对于任意一个神经元，限制其权值矩阵的L_2范数小于r,其中r为Max-norm超参数。实现的方法是\n",
    "##：w<-w*r/L_2(w),减小r可以防止过拟合，也可以缓解梯度消失爆炸之谜。tensorflow没有现成的\n",
    "##max-norm正则化工具，但可以以clip_weights进行\n",
    "\n",
    "threshold=1.0#即max-norm参数\n",
    "clipped_weights=tf.clip_by_norm(weights,clip_norm=threshold,axes=1)\n",
    "clip_weights=tf.assgin(weights,clipped_weights)\n",
    "#每次训练中都要执行该操作\n",
    "with tf.Session()as sess:\n",
    "    [...]\n",
    "    for epoch in range(n_epochs):\n",
    "        [...]\n",
    "        for X_batch,y_batch in zip(X_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch})\n",
    "            clip_weights.eval()\n",
    "#在模型设计阶段，还需连接每一层的weights参数。\n",
    "hidden1=fully_connected(X,n_hidden1,scope='hidden1')\n",
    "with tf.variable_scope('hidden1',reuse=True):\n",
    "    weight1=tf.get_variable('weights')\n",
    "#或者用根命名域\n",
    "hidden1=fully_connected(X,n_hidden1,scope='hidden1')\n",
    "hidden2=fully_connected(hidden1,n_hidden2,scope='hidden2')\n",
    "[...]\n",
    "with tf.variable_scope('',default_name='',reuse=True):#根命名域\n",
    "    weights1=tf.get_variable('hidden1/weights')\n",
    "    weights2=tf.get_variable('hidden2/weights')\n",
    "#如果变量的名称，你可以以tensorboard，或者调用global_variables()函数打印出变量的名字\n",
    "for variable in tf.global_variables():\n",
    "    print(variable.name)\n",
    "    \n",
    "\n",
    "    \n",
    "    #创建max_norm_regularizer()函数\n",
    "def max_norm_regularizer(threshold,axes=1,name='max_norm',collection='max_norm'):\n",
    "    def max_norm(weights):\n",
    "        clipped=tf.clip_by_norm(weights,clip_norm=threshold,axes=axes)\n",
    "        clip_weights=tf.assign(weights,clipped,name=name)\n",
    "        tf.add_to_collection(collection,clip_weights)\n",
    "        return None#没有正则损失项\n",
    "    return max_norm\n",
    "#然后就可以像调用其他正则器调用该函数\n",
    "max_norm_reg=max_norm_regularizer(threshold=1.0)\n",
    "hidden1=fully_connected(X,n_hidden1,scope='hidden1',weights_regularizer=max_norm_reg)\n",
    "#但仍需要每次训练都调用clip_weights操作，因此需要将clip_weights节点加入max_norm 修剪操作\n",
    "#的集合中，需要将修剪操作取出并每次运行之\n",
    "#\n",
    "clip_all_weights=tf.get_collection('max_norm')\n",
    "with tf.Session()as sess:\n",
    "    [...]\n",
    "    for epoch in range(n_epochs):\n",
    "        [...]\n",
    "        for X_batch,y_batch in zip(X_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化策略\n",
    "<img src='https://raw.githubusercontent.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/dev/images/chapter_11/t-11-1.jpg' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  调整，丢弃，替换高层网络\n",
    "##对于新的训练任务，输出层与高层通常是无用的，因此通常需要调整和替换。一般的步骤是：\n",
    "###封存所有复用层，训练观察其表现，然后从高层开始解封，通过反向传播调整高层网络。\n",
    "##如果表现不好，而训练样本也不多，则尝试将高层网络丢弃并封存余下的隐层，然后迭代进行\n",
    "##这个过程直至得的最优的复用网络数目。若训练样本足够多，那么你可以尝试替换隐层网络而非\n",
    "##丢弃他们，甚至还可以增加隐层。\n",
    "\n",
    "\n",
    "##  Model Zoos\n",
    "##https://github.com/tensorflow/models\n",
    "##https://github.com/ethereon/caffe-tensorflow\n",
    "\n",
    "\n",
    "##  无监督预训练\n",
    "##用受限玻尔兹曼机和自编码网络对网络进行无监督的预训练，然后利用有标签数据进行训练。 \n",
    "##在梯度消失问题得到缓解之后，无监督预训练才渐渐被反向传播替代。但基于子自编码网络的\n",
    "##预训练仍是一个有效的训练手段，尤其是当没有可复用的模型以及有标签的数据太少时。\n",
    "\n",
    "\n",
    "## 作为辅助任务的预训练\n",
    "##训练深度神经网络的最后一个方法是先训练一个易获得标签的、辅助任务的神经网络，然后复用\n",
    "##底层网络。例如，建立人脸识别系统问题，你不大可能一开始就拥有足够的样本，但你可以在网络\n",
    "##上搜集大量人脸图片，然后训练深度神经网络以识别任意两个图片是否是同一个人。然后复用其底层网络，建立自己的人脸识别系统。\n",
    "\n",
    "##将所有训练样本标记为‘好’，然后生成‘坏’样本，训练网络以识别二者。\n",
    "##另一种方法是训练神经网络对每个训练样本进行评分，用损失函数确保‘好’样本的得分高于‘坏’\n",
    "###样本至少一个边际，称之为最大边际学习。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/hzg0601/python/master/unsupervised_pretraining.png' widht='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  更快的优化器\n",
    "##加速训练的四种方法：Xavier/Glorot、He初始化策略；非饱和激活函数；批标准化；重用预训练的网络。\n",
    "  \n",
    "###另一种重要的方法使用更快的优化器：Momentum,Nesterov Accelerated Gradient,AdaGrad,\n",
    "###RMSProp,Adam,但通常情况下，最常用的优化器是AdamOptimizer，AdamOptimizer有三个超参数\n",
    "###需要优化，但使用默认参数效果已经非常好了。\n",
    "\n",
    "### Momentum优化器\n",
    "如保龄球沿着光滑而轻微倾斜的表面运动那样，开始非常缓慢，而一旦获得足够的动量，速度就好非常快。梯度优化的原则是：$\\theta=\\theta-\\eta \\Delta_{\\theta}J(\\theta)$,它并不关心初始的梯度是多少，如果初始梯度很小，那么训练速度回非常慢。<br/>\n",
    "  动量优化则不同，它的速度取决于前期的梯度是多少：每一次迭代中，它将局部梯度乘以学习率加入动量向量$\\boldsymbol{m}$中，然后以前一个参数减去动量向量更新参数。换句话说，梯度作用于加度数而非速度。为防止动量变得过大，引入超参数$\\beta$，简称为动量，默认值为0.9\n",
    "$$\\boldsymbol{m}\\leftarrow\\beta \\boldsymbol{m}+\\eta\\Delta_{\\theta}J(\\theta)$$\n",
    "$$\\theta\\leftarrow\\theta-\\boldsymbol{m}$$\n",
    "如果梯度值为常数，那么最终速度是梯度乘以学习率再乘以$\\frac{1}{1-\\beta}$,动量优化器可以使训练加速逃离平缓期。在不使用批标准化的深度网络中，高层网络通常会有不同的尺度，此时应用动量优化器将非常有效。<br/>\n",
    "如果没有摩擦系数$\\beta$，动量优化会在最优值附近震荡。\n",
    "\n",
    "### Nesterov Accelerated(Momentum) Gradient\n",
    "NAG/NMG的思路是：不是在原位置而是动量方向稍微向前的方向测度损失函数的梯度。这可以显著提高训练速度，原因在于动量向量通常会朝向最优的方向。\n",
    "$$\\boldsymbol{m}\\leftarrow\\beta \\boldsymbol{m}+\\eta\\Delta_{\\theta}J(\\theta+\\beta\\boldsymbol{m})$$\n",
    "$$\\theta\\leftarrow\\theta-\\boldsymbol{m}$$\n",
    "\n",
    "### AdaGrad\n",
    "如果算法能够提前侦知梯度曲线的斜率结构，如陡峭的部分之后是平坦的部分，那么速度可以进一步提升。AdaGrad优化器可以沿最陡峭的维度缩小梯度向量。\n",
    "$$\\boldsymbol{s}\\leftarrow\\boldsymbol{s}+\\Delta_{\\theta}J(\\theta)\\otimes\\Delta_{\\theta}J(\\theta)$$\n",
    "$$\\theta\\leftarrow\\theta-\\eta\\Delta_{\\theta}J(\\theta)\\oslash\\sqrt{\\boldsymbol{s}+\\epsilon}$$\n",
    "第一步将梯度的平方累加到$\\boldsymbol{s}$中，相当于$s_i\\leftarrow s_i+(\\partial/\\partial \\theta_iJ(\\theta))^2$.如若损失函数的沿着第i维陡峭，那么$s_i$将越来越大。<br/>\n",
    "第二步与梯度下降基本相同，唯一的不同在于，梯度向量按$\\sqrt{\\boldsymbol{s}+\\epsilon}$倍缩小，$\\oslash$代表按元素点除。$\\epsilon$为防止除数为0而设的小数。<br/>\n",
    "总而言之，这个算法的目的是为让学习率再训练中递减，它沿陡峭的维度收敛的比稍微倾斜的维度要快很多，也称为适应性学习率Adaptive learning rate.AdaGrad对二次型问题表现很好，但容易早停而陷入局部最优，因此不使用与深度网络。\n",
    "\n",
    "### RMSProp\n",
    "RMSProp通过开始时用指数衰减函数的方式只累加最近几次迭代的梯度以修正AdaGrad早停而陷入局部最优的缺陷。\n",
    "$$\\boldsymbol{s}\\leftarrow\\beta\\boldsymbol{s}+(1-\\beta)\\Delta_{\\theta}J(\\theta)\\otimes\\Delta_{\\theta}J(\\theta)$$\n",
    "$$\\theta\\leftarrow\\theta-\\eta\\Delta_{\\theta}J(\\theta)\\oslash\\sqrt{\\boldsymbol{s}+\\epsilon}$$\n",
    "在复杂问题中,RMSProp表现要优于AdaGrad.\n",
    "### Adam优化器\n",
    "Adam是自适应矩估计的代表，结合了Momentum和RMSProp的思路：它如Momentum一样追踪前向梯度的指数衰减平均值，如RMSProp一样追踪前向梯度平方的指数衰减平均值。\n",
    "$$\\boldsymbol{m}\\leftarrow\\beta_1\\boldsymbol{m}+(1-\\beta_1)\\Delta_{\\theta}J(\\theta)$$\n",
    "$$\\boldsymbol{s}\\leftarrow\\beta_2\\boldsymbol{s}+(1-beta_2)\\Delta_{\\theta}J(\\theta)\\otimes\\Delta_{\\theta}J(\\theta)$$\n",
    "$$\\boldsymbol{m}\\leftarrow\\frac{\\boldsymbol{m}}{1-\\beta_1^T}$$\n",
    "$$\\boldsymbol{s}\\leftarrow\\frac{\\boldsymbol{s}}{1-\\beta^T_2}$$\n",
    "$$\\theta\\leftarrow\\theta-\\eta\\boldsymbol{m}\\oslash\\sqrt{\\boldsymbol{s}+\\epsilon}$$\n",
    "其中，T代表迭代的次数。<br/>\n",
    "1和2应用的指数衰减平均值，而不是指数衰减和。$\\beta_1,beta_2$的初始值通常为0.9，和0.99，平滑项$\\epsilon$通常为$10^{-8}$,$\\eta$通常为0.001.<br/><br/>\n",
    "以上所用的全部优化器都是基于一次偏微分的（Jacobians），但理论上也可以基于二次偏微分（Hessians）.但由于计算复杂度的问题而难以应用。<br/>\n",
    "$l_1$正则化更易得的稀疏模型，但仅仅进行$l_1$正则化可能远远不够，因此需要Dual Averaging,称为Follow The Regularized Leader,Tensorflow提供FTRLOptimizer优化器实现FTRL-Proximal方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习率调整\n",
    "适应性学习率优化算法：AdaGrad,RMSProp,Adam.<br/>\n",
    "leraning schedules包括：\n",
    "### 先定分段常数学习率\n",
    "先设定一个较大的学习率，然后规定一定epoches之后变为另一个学习率。\n",
    "### 性能调整\n",
    "每n次计算一次验证误差，当验证误差不再减少，以参数$\\lambda$调整学习率。\n",
    "### 指数调整\n",
    "$\\eta(t)=\\eta_0 10^{-t/r}$,每10步调整一次学习率。\n",
    "### 幂调整\n",
    "$\\eta{t}=\\eta_0(1+t/r)^{-c}$,c通常设为1，比指数调整的缓慢些。<br/>\n",
    "性能调整和指数调整的表现最好，但指数调整更易于执行，调参，并且收敛也稍快。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化\n",
    "## 早停\n",
    "在验证集表现下降的时候停止。设置一个区间，如每50次，更新一次表现更好的模型，记录运行的次数，一旦表现更好的模型保存超过一定的运行次数，如200次，就停止运行。\n",
    "## $L_1$和$L_2$正则化\n",
    "在损失函数中加入正则项即可。\n",
    "## Dropout\n",
    "## 数据增益\n",
    "从现有数据中生成新数据，为数据增加**`可预测 `**的扰动。\n",
    "\n",
    "\n",
    "Default | DNN configration\n",
    "-|:-:|-:\n",
    "Initialization | He initialization\n",
    "Activation funtion  | ELU\n",
    "Normalization | Batch Normalization\n",
    "Regularizer | Dropout\n",
    "Optimizer | Adam\n",
    "Learning rate schedule | None\n",
    "\n",
    "* 如果' 不能 '寻找到最优的学习率，则需加入学习率调整策略。\n",
    "* 如果样本太少，可以引入数据增益。\n",
    "* 如果想得到稀疏模型可以引入$L_1$正则化，如果想得到更稀疏的模型，可以引入FTRL优化器替代Adam优化器，并加入$L_1$正则项。\n",
    "* 如果想获得快速实现的模型，则可以将批标准化步骤省去，将ELU替换为Leaky ReLU。\n",
    "\n",
    "**学习率调整**\n",
    "\n",
    "``` python\n",
    "\n",
    "initial_learning_rate=0.1\n",
    "dacay_steps=10000\n",
    "dacay_rate=1/10\n",
    "global_step=tf.Variable(0,trainable=False)\n",
    "learning_rate=tf.train.exponential_decay(initial_learning_rate,global_step,dacay_steps,\n",
    "                                        dacay_rate)\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate,momentum=0.9)\n",
    "training_op=optimizer.minimize(loss,global_step=global_step)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "he_init=tf.contrib.layers.variance_scaling_initializer()#He初始化，\n",
    "hidden1=tf.layers.dense(X,n_hidden1,activation=tf.nn.relu,\n",
    "                       kernel_initializer=he_init,name='hidden1')\n",
    "\n",
    "#ELU非饱和函数\n",
    "hidden1=tf.layers.dense(X,n_hidden1,activation=tf.nn.elu,name='hidden1')\n",
    "\n",
    "def leaky_relu(z,name=None):\n",
    "    return tf.maximum(0.01*z,z,name=name)\n",
    "hidden1=tf.layers.dense(X,n_hidden1,activation=leaky_relu,name='hidden1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GDN',\n",
       " 'OPTIMIZER_CLS_NAMES',\n",
       " 'OPTIMIZER_SUMMARIES',\n",
       " 'RevBlock',\n",
       " 'SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_allowed_symbols',\n",
       " 'apply_regularization',\n",
       " 'avg_pool2d',\n",
       " 'avg_pool3d',\n",
       " 'batch_norm',\n",
       " 'bias_add',\n",
       " 'bow_encoder',\n",
       " 'bucketized_column',\n",
       " 'check_feature_columns',\n",
       " 'conv2d',\n",
       " 'conv2d_in_plane',\n",
       " 'conv2d_transpose',\n",
       " 'conv3d',\n",
       " 'conv3d_transpose',\n",
       " 'convolution2d',\n",
       " 'convolution2d_in_plane',\n",
       " 'convolution2d_transpose',\n",
       " 'convolution3d',\n",
       " 'convolution3d_transpose',\n",
       " 'create_feature_spec_for_parsing',\n",
       " 'crossed_column',\n",
       " 'dense_to_sparse',\n",
       " 'dropout',\n",
       " 'elu',\n",
       " 'embed_sequence',\n",
       " 'embedding_column',\n",
       " 'embedding_lookup_unique',\n",
       " 'feature_column',\n",
       " 'flatten',\n",
       " 'fully_connected',\n",
       " 'gdn',\n",
       " 'group_norm',\n",
       " 'images_to_sequence',\n",
       " 'infer_real_valued_columns',\n",
       " 'input_from_feature_columns',\n",
       " 'instance_norm',\n",
       " 'joint_weighted_sum_from_feature_columns',\n",
       " 'l1_l2_regularizer',\n",
       " 'l1_regularizer',\n",
       " 'l2_regularizer',\n",
       " 'layer_norm',\n",
       " 'legacy_fully_connected',\n",
       " 'legacy_linear',\n",
       " 'legacy_relu',\n",
       " 'linear',\n",
       " 'make_place_holder_tensors_for_base_features',\n",
       " 'max_pool2d',\n",
       " 'max_pool3d',\n",
       " 'maxout',\n",
       " 'multi_class_target',\n",
       " 'one_hot_column',\n",
       " 'one_hot_encoding',\n",
       " 'optimize_loss',\n",
       " 'parse_feature_columns_from_examples',\n",
       " 'parse_feature_columns_from_sequence_examples',\n",
       " 'real_valued_column',\n",
       " 'recompute_grad',\n",
       " 'regression_target',\n",
       " 'relu',\n",
       " 'relu6',\n",
       " 'repeat',\n",
       " 'rev_block',\n",
       " 'safe_embedding_lookup_sparse',\n",
       " 'scale_gradient',\n",
       " 'scattered_embedding_column',\n",
       " 'separable_conv2d',\n",
       " 'separable_convolution2d',\n",
       " 'sequence_input_from_feature_columns',\n",
       " 'sequence_to_images',\n",
       " 'shared_embedding_columns',\n",
       " 'softmax',\n",
       " 'sparse_column_with_hash_bucket',\n",
       " 'sparse_column_with_integerized_feature',\n",
       " 'sparse_column_with_keys',\n",
       " 'sparse_column_with_vocabulary_file',\n",
       " 'spatial_softmax',\n",
       " 'stack',\n",
       " 'sum_regularizer',\n",
       " 'summaries',\n",
       " 'summarize_activation',\n",
       " 'summarize_activations',\n",
       " 'summarize_collection',\n",
       " 'summarize_tensor',\n",
       " 'summarize_tensors',\n",
       " 'transform_features',\n",
       " 'unit_norm',\n",
       " 'variance_scaling_initializer',\n",
       " 'weighted_sparse_column',\n",
       " 'weighted_sum_from_feature_columns',\n",
       " 'xavier_initializer',\n",
       " 'xavier_initializer_conv2d']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#批标准化\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "\n",
    "#使用partial（）函数简化代码\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_inputs = 28 * 28\n",
    "    n_hidden1 = 300\n",
    "    n_hidden2 = 100\n",
    "    n_outputs = 10\n",
    "\n",
    "    mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "    batch_norm_momentum = 0.9\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name = 'X')\n",
    "    y = tf.placeholder(tf.int64, shape=None, name = 'y')\n",
    "    training = tf.placeholder_with_default(False, shape=(), name = 'training')#给Batch norm加一个placeholder\n",
    "\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "        #对权重的初始化\n",
    "\n",
    "        my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training = training,\n",
    "            momentum = batch_norm_momentum\n",
    "        )\n",
    "\n",
    "        my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer = he_init\n",
    "        )#包含重复参数\n",
    "\n",
    "        hidden1 = my_dense_layer(X ,n_hidden1 ,name = 'hidden1')\n",
    "        bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "        hidden2 = my_dense_layer(bn1, n_hidden2, name = 'hidden2')\n",
    "        bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "        logists_before_bn = my_dense_layer(bn2, n_outputs, name = 'outputs')\n",
    "        logists = my_batch_norm_layer(logists_before_bn)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits= logists)\n",
    "        loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logists, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    n_epoches = 20\n",
    "    batch_size = 200\n",
    "# 注意：由于我们使用的是 tf.layers.batch_normalization() 而不是 tf.contrib.layers.batch_norm()（如本书所述），\n",
    "# 所以我们需要明确运行批量规范化所需的额外更新操作（sess.run([ training_op，extra_update_ops], ...)。\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epoches):\n",
    "            for iteraton in range(mnist.train.num_examples//batch_size):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                sess.run([training_op,extra_update_ops],\n",
    "                         feed_dict={training:True, X:X_batch, y:y_batch})\n",
    "            accuracy_val = accuracy.eval(feed_dict= {X:mnist.test.images,\n",
    "                                                    y:mnist.test.labels})\n",
    "            print(epoch, 'Test accuracy:', accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#也可以将train阶段改为，而替代sess.run([training_op,extra_update_ops]..)为sess.run(training_op,....)\n",
    "with tf.name_scope('train'):\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    extra_update_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op=optimizer.minimizer(loss)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#梯度修剪\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#复用tensorflow模型\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#只复用部分结果\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):                                      # not shown in the book\n",
    "        for iteration in range(mnist.train.num_examples // batch_size): # not shown\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)      # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})  # not shown\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,  # not shown\n",
    "                                                y: mnist.test.labels}) # not shown\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)                   # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学习率调整\n",
    " initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "with tf.name_scope(\"train\"):       # not shown in the book\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L_1和L_2正则化\n",
    "scale=0.01\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):                                     # not shown in the book\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown\n",
    "        labels=y, logits=logits)                                # not shown\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # not shown\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大范数正则化\n",
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)\n",
    "\n",
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,     # not shown in the book\n",
    "                                            y: mnist.test.labels})    # not shown\n",
    "        print(epoch, \"Test accuracy:\", acc_test)                      # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")             # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "mnist_raw=loadmat('C:\\\\Users\\\\hzg0601\\\\datasets\\\\mnist-original.mat')\n",
    "mnist = {\n",
    "        \"data\": mnist_raw[\"data\"].T,\n",
    "        \"target\": mnist_raw[\"label\"][0],\n",
    "        \"COL_NAMES\": [\"label\", \"data\"],\n",
    "        \"DESCR\": \"mldata.org dataset: mnist-original\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
