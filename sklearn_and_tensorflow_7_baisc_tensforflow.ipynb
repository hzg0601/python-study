{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow\n",
    "\n",
    "##首先定义计算图，然后交给C++执行\n",
    "##支持并行计算；支持分布式计算。\n",
    "#跨平台；\n",
    "#简单的API:tensorflow.contrib.learn;skflow;tensorflow.contrib.slim\n",
    "#高级封装：Keras;Pretty Tensor\n",
    "#支持灵活的结构设计\n",
    "#结合C++的高执行效率\n",
    "#支持先进的优化方法:支持automatic differentiating\n",
    "#支持tensorboard等可视化工具\n",
    "#连接谷歌云服务\n",
    "#强大的社区支持。\n",
    "\n",
    "\n",
    "\n",
    "#定义计算图\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x=tf.Variable(3,name='x')\n",
    "y=tf.Variable(4,name='y')\n",
    "f=x*x*y+y+2\n",
    "\n",
    "#开启会话执行\n",
    "sess=tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result=sess.run(f)\n",
    "print(result)\n",
    "sess.close()\n",
    "\n",
    "#利用with管理会话\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()#相当于调用tf.get_default_session().run(x.initializer)\n",
    "    y.initializer.run()\n",
    "    result=f.eval()#相当于调用tf.get_defualt_session().run(f)\n",
    "    \n",
    "#利用tf.global_variables_initializer()开启全局初始化\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result=f.eval()\n",
    "    \n",
    "#开启InterativeSession，自动将其设为默认会话，不必启用with\n",
    "sess=tf.InterativeSession()\n",
    "init.run()\n",
    "result=f.eval()\n",
    "print(result)\n",
    "sess.close()\n",
    "\n",
    "#执行阶段常以循环的方式进行训练\n",
    "\n",
    "\n",
    "#管理图\n",
    "#所有创建的节点都会自动加入默认图\n",
    "x1=tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()\n",
    "\n",
    "#若要创建多个图，用with关键词\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2=tf.Variable(2)\n",
    "x2.graph is graph\n",
    "    \n",
    "x2.praph is tf.get_default_graph()\n",
    "\n",
    "#若多次运行同一些命令，须重启graph，即执行tf.reset_default_graph()\n",
    "\n",
    "#计算节点值时，tensorflow自动确定节点间的依赖关系\n",
    "w=tf.constant(3)\n",
    "x=w+2\n",
    "y=x+5\n",
    "z=x*3\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval)\n",
    "    print(z.eval)#用eval会自动计算w，x两次\n",
    "#节点值在计算后就会被丢弃，但变量不会，会保留到会话结束。\n",
    "\n",
    "#用run可以让tf一次性计算y,z的值\n",
    "with tf.Session() as sess:\n",
    "    y_val,z_val=sess.run([y,z])\n",
    "    print(y_val)\n",
    "    print(z_val)\n",
    "#在单过程的tf，多个会话间并不共享其状态，即使是复用相同的图；但在分布式tensorflow中，变量\n",
    "#状态存储在服务器中而非会话中，因此，多个会话间可以共享变量状态。\n",
    "\n",
    "#Linear Regression\n",
    "#算子可以处理多个输入输出，Constant和Varable不需要输入，称为源算子。而非源算子的输入输出是\n",
    "#多维数组，称为tensor，Python API中的tensor可有NumPy的数组表示。\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing=fetch_california_housing()\n",
    "m,n=housing.data.shape\n",
    "housing_data_plus_bias=np.c_[np.ones((m,1)),housing.data]\n",
    "X=tf.constant(housing_data_plus_bias,dtype=tf.float32,name='X')\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name='y')\n",
    "XT=tf.transpose(X)\n",
    "theta=tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)),XT),y)\n",
    "with tf.Session() as sess:\n",
    "    theta_value=theta.eval()\n",
    "    theta_run=sess.run(theta)\n",
    "    print(theta_run)\n",
    "    print(theta_value)\n",
    "    \n",
    "#手动执行梯度下降\n",
    "n_epchs=1000\n",
    "learning_rate=0.01\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name='X')\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name='y')\n",
    "theta=tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name='theta')\n",
    "y_pred=tf.matmul(X,theta,name='predictions')\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name='mse')\n",
    "gradients=2/m*tf.matmul(tf.transpose(X),error)\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch%100==0:\n",
    "            print('Epoch',epoch,'MSE',mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()\n",
    "\n",
    "#自动执行符号微分，从而进行梯度下降\n",
    "gradients=tf.gradients(mse,[theta])[0]\n",
    "#返回算子对变量的列表（如果有多个变量）\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch%100==0:\n",
    "            print('Epoch',epoch,'MSE',mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()\n",
    "\n",
    "#添加优化算子\n",
    "gradients=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "training_op=optimizer.minimize(mse)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch%100==0:\n",
    "            print('Epoch',epoch,'MSE',mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()\n",
    "    \n",
    "    \n",
    "    \n",
    "from datetime import datetime\n",
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir='tf_logs'\n",
    "logdir='{}/run-{}/'.format(root_logdir,now)\n",
    "#为每次运行的结果设一个路径，以防多次运行加载到tensorboard时错误\n",
    "\n",
    "    \n",
    "#使用自定义优化算子\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "gradients=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "training_op=optimizer.minimize(mse)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "saver=tf.train.Saver({'weight':theta})#将所有参数初始化后保存所有节点\n",
    "#可以重命名节点\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    #如果再次加载，则需重新加载已保存的节点\n",
    "    saver.restore(sess,'/my_final_model.ckpt')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch%100==0:\n",
    "            save_path=saver.save(sess,'/my_model.ckpt')\n",
    "            print('Epoch',epoch,'MSE',mse.eval())#可以在任何时间点保存节点\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta=theta.eval()\n",
    "    save_path=saver.save(sess,'/my_final_model.ckpt')#以不同的节点命名不同的保存内容\n",
    "    mse_summary=tf.summary.scalar(\"MSE\",mse)#创建MSE的可视化节点，并写入summary的二进制tb文件\n",
    "    file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "    #将存储的节点写入文件，第二个参数是要可视化的部分。\n",
    "    #FileWriter会创建登录工作区，写入图的定义至事件文件。\n",
    "    \n",
    "\n",
    "#向训练算法添加数据\n",
    "#使用Mini-batch SGD,需要运用placeholder占位符节点，占位符不进行运算，而是返回数据以返回运行时\n",
    "#使用None表示任何形状\n",
    "A=tf.placeholder(tf.float32,shape=(None,3))\n",
    "B=A+5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1=B.eval(feed_dict={A:[[1,2,3]]})\n",
    "    B_val_2=B.eval(feed_dict={A:[[4,5,6],[7,8,9]]})\n",
    "    print(B_val_1)\n",
    "    print(B_val_2)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7185181e+01]\n",
      " [ 4.3633747e-01]\n",
      " [ 9.3952334e-03]\n",
      " [-1.0711310e-01]\n",
      " [ 6.4479220e-01]\n",
      " [-4.0338000e-06]\n",
      " [-3.7813708e-03]\n",
      " [-4.2348403e-01]\n",
      " [-4.3721911e-01]]\n",
      "[[-3.7185181e+01]\n",
      " [ 4.3633747e-01]\n",
      " [ 9.3952334e-03]\n",
      " [-1.0711310e-01]\n",
      " [ 6.4479220e-01]\n",
      " [-4.0338000e-06]\n",
      " [-3.7813708e-03]\n",
      " [-4.2348403e-01]\n",
      " [-4.3721911e-01]]\n"
     ]
    }
   ],
   "source": [
    "X=tf.palceholder(tf.float32,shape=(None,n+1),name='X')\n",
    "y=tf.placeholder(tf.float32,shape=(None,1),name='y')\n",
    "batch_size=100\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "def fetch_batch(epoch,batch_index,batch_size):\n",
    "    [...]#load data \n",
    "    return X_batch,y_batch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_pochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch,y_batch=fetch_batch(epoch,batch_index,batch_size)\n",
    "            if batch_index%10==0:\n",
    "                summary_str=mse_summary.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "                step=epoch*n_batches+batch_index\n",
    "                file_writer.add_summary(summary_str,step)\n",
    "                #可以在训练中每隔一段时间将数据保存下来\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch})\n",
    "            best_theta=theta.eval()\n",
    "            \n",
    "            file_writer.close()#必须结束写入命令\n",
    "#ls -l tf_log/run*查看文件\n",
    "#tensorboard 进入tensorboard\n",
    "\n",
    "#show_graph deepdream tutorial notebook\n",
    "#TensorFlow debugger tool\n",
    "\n",
    "#命名域\n",
    "#在处理复杂模型时，计算图中可能有很多节点，为避免这种状况，需定义命名域.\n",
    "#定义命名域后，节点将会在tensorboard中以二级菜单的方式隐藏\n",
    "with tf.name_scope('loss') as scope:\n",
    "    error=y_pred-y\n",
    "    mse=tf.reduce_mean(tf.square(error),name='mse')\n",
    "    \n",
    "#模块化modularity\n",
    "n_features=3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_feature),name='X')\n",
    "w1=tf.Variable(tf.ramdom_normal((n_feature,1)),name='weights1')\n",
    "w2=tf.Variable(tf.random_normal((n_feature,1)),name='weights2')\n",
    "b1=tf.Variable(0.0,name='bias1')\n",
    "b2=tf.Variable(0.0,name='bias2')\n",
    "z1=tf.add(tf.matmul(X,w1),b1,name='z1')\n",
    "z2=tf.add(tf.matmul(X,w2),b2,name='z2')\n",
    "relu1=tf.maximum(z1,0,name='relu1')\n",
    "relu2=tf.maximum(z2,0,name='relu2')\n",
    "output=tf.add(relu1,relu2,name='output')\n",
    "\n",
    "#通过模块化减少代码重复率\n",
    "#tensorflow会自动检查命名是否已出现，如未出现则命名之，已出现则加下标。\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope('relu'):#增加命名空间使可视化效果更简洁\n",
    "        w_shape=(int(X.get_shape()[1]),1)\n",
    "        w=tf.Variable(tf.random_normal(w_shape),name='weights')\n",
    "        b=tf.Variable(0.0,name='bias')\n",
    "        z=tf.add(matmul(X,w),b,name='z')\n",
    "        return tf.maximum(z,0.,name='relu')\n",
    "n_features=3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name='X')\n",
    "relus=[relu(X) for i in range(5)]\n",
    "output=tf.add_n(relus,name='output')#支持组内元素相加\n",
    "\n",
    "##共享变量\n",
    "#首先定义函数，再定义变量，将变量传给函数\n",
    "def relu(X,threshold):\n",
    "    with tf.name_scope('relu'):\n",
    "        w=tf.Variable(tf.random_normal(w_shape),name='weights')\n",
    "        b=tf.Variable(0.0,name='bias')\n",
    "        z=tf.add(matmul(X,w),b,name='z')\n",
    "        return tf.maximum(z,threshold,name='relu')\n",
    "threshold=tf.Variable(0.0,name='threshold')\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name='X')\n",
    "relus=[relu(X,threshold) for i in range(5)]\n",
    "output=tf.add_n(relus,name='output')\n",
    "\n",
    "#但以上方法必须每次都有传递参数，为避免这种状况。一种方法是将模型中的变量定义变量字典，并\n",
    "#传递给所有函数。另一种方法是为每个模块定义一个类，还有一种方法是将共享变量定义成函数的属性。\n",
    "def relu(X):\n",
    "    with tf.name_scope('relu'):\n",
    "        if not hasattr(relu,;threshold):\n",
    "            relu.threshold=tf.Variable(0.0,name='threshold')\n",
    "            w_shape = int(X.get_shape()[1]), 1                          # not shown in the book\n",
    "            w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "            b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "            z = tf.add(tf.matmul(X, w), b, name=\"z\")   \n",
    "            return tf.maximum(z,relu.threshold,name='max')\n",
    "#另一种比较常用的方法是get_variable()函数创建共享变量，如果不存在则创建；如果存在则重新调用\n",
    "#该功能由variable_scope属性控制。\n",
    "with tf.variable_scope('relu'):\n",
    "    threshold=tf.get_variable('threshold',shape=(),initilizer=tf.constant_initilizer(0.0))\n",
    "#如果变量已被get_variable(）定义，该方法会报错；如果要复用，则需要在定义变量域时申明可复用\n",
    "with tf.variable_scope('relu',reuse=True):\n",
    "    threshold=tf.get_variable('threshold')\n",
    "#但如果变量不存在，或者不是由get_varialbe()定义，该方法会报错。\n",
    "#也可以将reuse属性定义在variable_scope内部\n",
    "with tf.variable_scope('relu') as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold=tf.get_variable('threshold')\n",
    "#一旦reuse被设为True，就不可设为False，如果在一个variable scope定义其他variable scope，\n",
    "#其他variable scope会自动继承reuse=True,只有get_variable（）定义的参数才能这样用。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先定义函数，函数中声名可以重用已存在变量，然后先定义变量，再调用函数。\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope('relu',reuse=True):\n",
    "        threshold=tf.get_variable('threshold')#重用已存在变量\n",
    "        w_shape = int(X.get_shape()[1]), 1 \n",
    "        w=tf.Variable(tf.random_normal(w_shape),name='weights')\n",
    "        b=tf.Variable(0.0,name='bias')\n",
    "        z=tf.add(tf.matmul(X,w),b,name='z')\n",
    "        return tf.maximum(z,threshold,name='max')\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name='X')\n",
    "with tf.variable_scope('relu'):#创建变量\n",
    "    threshold=tf.get_variable('threshold',shape=(),initializer=tf.constant_initializer(0.0))\n",
    "relus=[relu(X) for relu_index in range(5)]\n",
    "output=tf.add_n(relus,name='output')\n",
    "    #首先定义relu函数，然后创建relu/threshold变量，创建五个ReLU，relu()重用relu/threshold\n",
    "    #变量，并创建其他ReLU节点。\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一些情况下，我们可能必须在relu之外定义threshold，而ReLU节点定义在relu()内部。\n",
    "#以下代码在第一次调用时在relu()函数中创建threshold变量，在之后的调用时复用。\n",
    "#relu()函数之后就可以调用get_variable()以调用或复用threshold变量。\n",
    "#需控制在第一次调用时reuse=False\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "    threshold=tf.get_variable('threshold',shape=(),initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w=tf.Variable(tf.random_normal(w_shape),name='weights')\n",
    "    b=tf.Variable(0.0,name='bias')\n",
    "    z=tf.add(tf.matmul(X,w),b,name='z')\n",
    "    return tf.maximum(z,threshold,name='max')\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name='X')\n",
    "relus=[]\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope('relu',reuse=(relu_index>=1)) as scope:\n",
    "        \n",
    "        relus.append(relu(X))\n",
    "output=tf.add_n(relus,name='output')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "    with tf.variable_scope('relu') as scope:\n",
    "        scope.reuse_variables()\n",
    "        threshold=tf.get_variable('threshold')#重用已存在变量\n",
    "        w=tf.Variable(tf.random_normal((n_features,1)),name='weights')\n",
    "        b=tf.Variable(0.0,name='bias')\n",
    "        z=tf.add(tf.matmul(X,w),b,name='z')\n",
    "        return tf.maximum(z,threshold,name='max')\n",
    "n_features=3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name='X')\n",
    "\n",
    "with tf.variable_scope('relu'):#创建变量\n",
    "    \n",
    "    threshold=tf.get_variable('threshold',shape=())\n",
    "relus=[relu(X) for relu_index in range(5)]\n",
    "output=tf.add_n(relus,name='output')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
